{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87cd8ba-7d1f-4c76-a2a0-536e7e93ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f03cdd-6548-42f7-b8a1-340c0b7472fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/connor/code/zulu-tango/news_and_echo_bubbles/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90bb7328-7be7-4cbf-8a46-60f5563e131c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/connor/code/zulu-tango/news_and_echo_bubbles'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent = os.path.dirname(cwd)\n",
    "parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee94ecb-1beb-4df8-ba3a-3b4e69451309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/connor/code/zulu-tango/news_and_echo_bubbles/raw_data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.path.join(parent,\"raw_data\")\n",
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd46ba3-a287-4117-9558-4f946b924d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_data_location = os.path.join(data_folder,\"braindedleft.csv\")\n",
    "right_data_location = os.path.join(data_folder,\"braindedright.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3291a6f6-b8ad-4f9d-8087-6d98a36f6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in left and right datasets\n",
    "df_left = pd.read_csv(left_data_location)\n",
    "df_right = pd.read_csv(right_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f1110d-3a9c-47a7-9563-1e50b70c68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a 0 to left wing articles and a 1 to right wing articles\n",
    "df_left[\"ideology\"] = 0\n",
    "df_right[\"ideology\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ef0133-dde0-4a26-b377-4c7400b9dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two datasets\n",
    "merged_df = pd.concat([df_left,df_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18769b1-43a6-4c54-a6e2-10cda2156499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>link</th>\n",
       "      <th>pdate</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>tags</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>ideology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://kindest.com/442355-defend-democracy-to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Defend Democracy Today: Support Fearless, Insi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>While the election has come and gone, the figh...</td>\n",
       "      <td>['reporting', 'come', 'today', 'access', 'chec...</td>\n",
       "      <td>set()</td>\n",
       "      <td>0.8612</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.165589</td>\n",
       "      <td>0.539782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               link pdate  \\\n",
       "0           0  https://kindest.com/442355-defend-democracy-to...   NaN   \n",
       "\n",
       "                                               title author  \\\n",
       "0  Defend Democracy Today: Support Fearless, Insi...     []   \n",
       "\n",
       "                                                text  \\\n",
       "0  While the election has come and gone, the figh...   \n",
       "\n",
       "                                            keywords   tags  compound    neg  \\\n",
       "0  ['reporting', 'come', 'today', 'access', 'chec...  set()    0.8612  0.063   \n",
       "\n",
       "     neu    pos  polarity  subjectivity  ideology  \n",
       "0  0.819  0.118  0.165589      0.539782         0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7883f3e-672c-4094-b4e4-36257d4911cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>link</th>\n",
       "      <th>pdate</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>tags</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>ideology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>1809</td>\n",
       "      <td>https://americanmind.org/video/book-talk-crisi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The American Mind Podcast: The Roundtable Epis...</td>\n",
       "      <td>['Christopher Buskirk', 'Kevin Portteus', 'Dan...</td>\n",
       "      <td>The American Mind’s ‘Editorial Roundtable’ pod...</td>\n",
       "      <td>['mind', 'williams', 'trump', 'american', 'unc...</td>\n",
       "      <td>set()</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               link pdate  \\\n",
       "1809        1809  https://americanmind.org/video/book-talk-crisi...   NaN   \n",
       "\n",
       "                                                  title  \\\n",
       "1809  The American Mind Podcast: The Roundtable Epis...   \n",
       "\n",
       "                                                 author  \\\n",
       "1809  ['Christopher Buskirk', 'Kevin Portteus', 'Dan...   \n",
       "\n",
       "                                                   text  \\\n",
       "1809  The American Mind’s ‘Editorial Roundtable’ pod...   \n",
       "\n",
       "                                               keywords   tags  compound  \\\n",
       "1809  ['mind', 'williams', 'trump', 'american', 'unc...  set()    0.6808   \n",
       "\n",
       "        neg  neu    pos  polarity  subjectivity  ideology  \n",
       "1809  0.026  0.9  0.074  0.114815      0.296296         1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50039cca-526c-4b9d-bdab-1201bc62e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "# we may not want to drop all of these for later analysis, but this is fine for now\n",
    "merged_df = merged_df.drop(columns = [\"Unnamed: 0\", \"link\", \"pdate\", \"title\", \"author\",\t\"keywords\",\\\n",
    "                          \"tags\", \"compound\", \"neg\", \"neu\", \"pos\", \"polarity\", \"subjectivity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6d4e438-a704-42de-a3ec-6b3b8b20da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a sample to make testing the model easier\n",
    "df_sample = merged_df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af53be7f-165a-4388-a5f9-48510e0999c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data is balanced between left-wing and right-wing (i.e. c.250 of each class)\n",
    "df_sample[\"ideology\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2705602-84f5-4094-96b7-fd22fa56effc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: transformers in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (1.26.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: filelock in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/connor/.pyenv/versions/3.10.6/envs/news_and_echo_bubbles/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef159519-65b3-497f-af22-c7d95d61d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 06:14:56.918022: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-28 06:14:56.979608: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-28 06:14:57.320062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-28 06:14:57.320134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-28 06:14:57.379866: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-28 06:14:57.545122: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-28 06:14:57.546951: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-28 06:14:58.946311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd2e58e-57a1-47c3-9ce1-3f34ad30fc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ideology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>The United Nations body responsible for overse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Criminal Complaint\\n\\nA retired Ohio couple wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>“We’re now getting more of these MIS-C kids, b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Senate Democrats are pushing to reform the fil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>One reason Trump supporters believe his lies c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  ideology\n",
       "570  The United Nations body responsible for overse...         1\n",
       "768  Criminal Complaint\\n\\nA retired Ohio couple wh...         0\n",
       "482  “We’re now getting more of these MIS-C kids, b...         0\n",
       "69   Senate Democrats are pushing to reform the fil...         0\n",
       "359  One reason Trump supporters believe his lies c...         0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1317679-9aa1-4be5-9b3b-a5c87334c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what the lengths of the texts are\n",
    "# need to convert texts to string first before calculating length, as I was getting an error that some texts were float values and not strings\n",
    "# this should be fixed during pre-processing\n",
    "# using a basic split by spaces and no other delimiters\n",
    "df_sample[\"text_length\"] = df_sample[\"text\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "676f1b35-4d07-406f-9b6e-bf6eea71caea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ideology</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>The United Nations body responsible for overse...</td>\n",
       "      <td>1</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Criminal Complaint\\n\\nA retired Ohio couple wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>“We’re now getting more of these MIS-C kids, b...</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Senate Democrats are pushing to reform the fil...</td>\n",
       "      <td>0</td>\n",
       "      <td>2491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>One reason Trump supporters believe his lies c...</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  ideology  text_length\n",
       "570  The United Nations body responsible for overse...         1          659\n",
       "768  Criminal Complaint\\n\\nA retired Ohio couple wh...         0         1545\n",
       "482  “We’re now getting more of these MIS-C kids, b...         0          304\n",
       "69   Senate Democrats are pushing to reform the fil...         0         2491\n",
       "359  One reason Trump supporters believe his lies c...         0          560"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d862ca5-7908-4ace-82f1-4fc0cb6de80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ideology</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>The United Nations body responsible for overse...</td>\n",
       "      <td>1</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Criminal Complaint\\n\\nA retired Ohio couple wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>“We’re now getting more of these MIS-C kids, b...</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Senate Democrats are pushing to reform the fil...</td>\n",
       "      <td>0</td>\n",
       "      <td>2491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>One reason Trump supporters believe his lies c...</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Glyphosate is a systemic broad-spectrum herbic...</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>100 200 300 400 million Dec. 13 Mar. 21 448 mi...</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>For a long time, we are told, capitalism has b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>Myanmar Coup: With Aung San Suu Kyi Detained, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>\"We proudly handed the Biden administration th...</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  ideology  text_length\n",
       "570   The United Nations body responsible for overse...         1          659\n",
       "768   Criminal Complaint\\n\\nA retired Ohio couple wh...         0         1545\n",
       "482   “We’re now getting more of these MIS-C kids, b...         0          304\n",
       "69    Senate Democrats are pushing to reform the fil...         0         2491\n",
       "359   One reason Trump supporters believe his lies c...         0          560\n",
       "...                                                 ...       ...          ...\n",
       "407   Glyphosate is a systemic broad-spectrum herbic...         0          299\n",
       "2344  100 200 300 400 million Dec. 13 Mar. 21 448 mi...         0          800\n",
       "1488  For a long time, we are told, capitalism has b...         0         1195\n",
       "1200  Myanmar Coup: With Aung San Suu Kyi Detained, ...         1          861\n",
       "1351  \"We proudly handed the Biden administration th...         1          258\n",
       "\n",
       "[93 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df_sample.dropna()\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52aac8a1-653e-42f9-b94b-89e62c4629cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37 rows were dropped in this, therefore there must have been many NaN values\n",
    "# these will be dropped during preprocessing, so this is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea93ae67-0c04-4e6d-9956-a37be26df479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "177df2e2-95ca-4e1d-a5fc-b371cc5c512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc/UlEQVR4nO3df3DX9X3A8VcgJMAkQQQSUsMvsTh/QCdWmlWdHTmBcU5b/1DH7dB5enW4q6PVQrdq2XYX1t051x61u9sq690qW3cVt9KyURRYt4CFSpHaMmEwcBJocUkAJSJ57w+Pb/sVEKP5vpNvfDzuvnfk+3nz+b5ffoJ53jffb1KRUkoBAJDJoL7eAADw/iI+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgq8q+3sBbdXd3x8svvxwjRoyIioqKvt4OAPAOpJTiyJEj0dDQEIMGvf1zG/0uPl5++eVobGzs620AAO/C/v3748ILL3zbNf0uPkaMGBERb26+pqamj3cDALwTnZ2d0djYWPg6/nb6XXyc+lZLTU2N+ACAMvNOXjLhBacAQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKwq+3oDA8XExatLdu69y+aV7NwAkJtnPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAsupRfLS0tMSHP/zhGDFiRIwdOzZuvvnm2LlzZ9Ga48ePx8KFC+OCCy6I8847L2655ZY4ePBgr24aAChfPYqPDRs2xMKFC2PTpk2xdu3aOHHiRNxwww1x7Nixwpo//MM/jH/5l3+Jb37zm7Fhw4Z4+eWX4xOf+ESvbxwAKE+VPVm8Zs2aoo9XrFgRY8eOja1bt8Z1110XHR0d8bd/+7fxjW98I37zN38zIiIef/zx+NVf/dXYtGlTfOQjH+m9nQMAZek9veajo6MjIiJGjRoVERFbt26NEydORHNzc2HNJZdcEuPHj4/W1tYznqOrqys6OzuLbgDAwPWu46O7uzvuv//++OhHPxqXX355RES0tbVFVVVVjBw5smhtXV1dtLW1nfE8LS0tUVtbW7g1Nja+2y0BAGXgXcfHwoULY8eOHbFy5cr3tIElS5ZER0dH4bZ///73dD4AoH/r0Ws+Trnvvvvi29/+dmzcuDEuvPDCwv319fXx+uuvR3t7e9GzHwcPHoz6+voznqu6ujqqq6vfzTYAgDLUo2c+Ukpx3333xZNPPhlPP/10TJo0qej4jBkzYsiQIbFu3brCfTt37ox9+/ZFU1NT7+wYAChrPXrmY+HChfGNb3wjnnrqqRgxYkThdRy1tbUxbNiwqK2tjbvuuisWLVoUo0aNipqamviDP/iDaGpq8k4XACAiehgfjz32WEREXH/99UX3P/7443HHHXdERMRf/uVfxqBBg+KWW26Jrq6umD17dnzlK1/plc0CAOWvR/GRUjrnmqFDh8by5ctj+fLl73pTAMDA5Xe7AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFlV9vUGcpu4eHVfbwEA3tc88wEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkFWP42Pjxo1x4403RkNDQ1RUVMSqVauKjt9xxx1RUVFRdJszZ05v7RcAKHM9jo9jx47F9OnTY/ny5WddM2fOnDhw4EDh9sQTT7ynTQIAA0dlT//C3LlzY+7cuW+7prq6Ourr69/1pgCAgaskr/lYv359jB07NqZOnRr33ntvHD58+Kxru7q6orOzs+gGAAxcvR4fc+bMia9//euxbt26+PM///PYsGFDzJ07N06ePHnG9S0tLVFbW1u4NTY29vaWAIB+pMffdjmX2267rfDnK664IqZNmxYXXXRRrF+/PmbNmnXa+iVLlsSiRYsKH3d2dgoQABjASv5W28mTJ8fo0aNj165dZzxeXV0dNTU1RTcAYOAqeXy89NJLcfjw4Rg3blypHwoAKAM9/rbL0aNHi57F2LNnT2zbti1GjRoVo0aNiqVLl8Ytt9wS9fX1sXv37njwwQdjypQpMXv27F7dOABQnnocH1u2bImPfexjhY9PvV5jwYIF8dhjj8X27dvj7/7u76K9vT0aGhrihhtuiD/90z+N6urq3ts1AFC2ehwf119/faSUznr8X//1X9/ThgCAgc3vdgEAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALKq7OsNcG4TF68u2bn3LptXsnMDwJl45gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACCrHsfHxo0b48Ybb4yGhoaoqKiIVatWFR1PKcVDDz0U48aNi2HDhkVzc3O8+OKLvbVfAKDM9Tg+jh07FtOnT4/ly5ef8fgXv/jF+NKXvhRf/epXY/PmzfErv/IrMXv27Dh+/Ph73iwAUP56/Ftt586dG3Pnzj3jsZRSPProo/HHf/zHcdNNN0VExNe//vWoq6uLVatWxW233fbedgsAlL1efc3Hnj17oq2tLZqbmwv31dbWxsyZM6O1tfWMf6erqys6OzuLbgDAwNXjZz7eTltbW0RE1NXVFd1fV1dXOPZWLS0tsXTp0t7cBj0wcfHqkpx377J5JTkvAOWvz9/tsmTJkujo6Cjc9u/f39dbAgBKqFfjo76+PiIiDh48WHT/wYMHC8feqrq6OmpqaopuAMDA1avxMWnSpKivr49169YV7uvs7IzNmzdHU1NTbz4UAFCmevyaj6NHj8auXbsKH+/Zsye2bdsWo0aNivHjx8f9998ff/ZnfxYXX3xxTJo0KT7/+c9HQ0ND3Hzzzb25bwCgTPU4PrZs2RIf+9jHCh8vWrQoIiIWLFgQK1asiAcffDCOHTsW99xzT7S3t8c111wTa9asiaFDh/bergGAslWRUkp9vYlf1tnZGbW1tdHR0VGS13+U6t0dFPNuF4D3l558/e7zd7sAAO8v4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFaVfb0B6C8mLl5dsnPvXTavZOcGKDee+QAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsKvt6AwxMExevLtm59y6bV7JzA1B6nvkAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDIqtfj4wtf+EJUVFQU3S655JLefhgAoEyV5BfLXXbZZfG9733vFw9S6ffXAQBvKkkVVFZWRn19fSlODQCUuZK85uPFF1+MhoaGmDx5csyfPz/27dt31rVdXV3R2dlZdAMABq5ef+Zj5syZsWLFipg6dWocOHAgli5dGtdee23s2LEjRowYcdr6lpaWWLp0aW9vgwFs4uLVfb0FAN6DipRSKuUDtLe3x4QJE+KRRx6Ju+6667TjXV1d0dXVVfi4s7MzGhsbo6OjI2pqanp9P75w0Rf2LpvX11sAKKnOzs6ora19R1+/S/5K0JEjR8YHP/jB2LVr1xmPV1dXR3V1dam3AQD0EyX/OR9Hjx6N3bt3x7hx40r9UABAGej1+PjMZz4TGzZsiL1798Z//ud/xsc//vEYPHhw3H777b39UABAGer1b7u89NJLcfvtt8fhw4djzJgxcc0118SmTZtizJgxvf1QAEAZ6vX4WLlyZW+fEgAYQPxuFwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgq8q+3gDQP01cvLqvt9Bje5fN6+stAO+AZz4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgq8q+3gC8H0xcvLqvtwDva6X8N7h32bySnXug8swHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZFXZ1xsA6O8mLl5dsnPvXTavZOcu1b5LuWd+oVw/794Jz3wAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkVbL4WL58eUycODGGDh0aM2fOjGeffbZUDwUAlJGSxMc//MM/xKJFi+Lhhx+OH/7whzF9+vSYPXt2HDp0qBQPBwCUkZLExyOPPBJ333133HnnnXHppZfGV7/61Rg+fHh87WtfK8XDAQBlpLK3T/j666/H1q1bY8mSJYX7Bg0aFM3NzdHa2nra+q6urujq6ip83NHRERERnZ2dvb21iIjo7nq1JOcF+l45/n+jVHuOKN2+S7nnUinHa1huez51zpTSOdf2enz8/Oc/j5MnT0ZdXV3R/XV1dfHTn/70tPUtLS2xdOnS0+5vbGzs7a0BA1zto329g56z5/JXjv89SrnnI0eORG1t7duu6fX46KklS5bEokWLCh93d3fHK6+8EhdccEFUVFS8p3N3dnZGY2Nj7N+/P2pqat7rVvuVgTxbxMCez2zlyWzlayDP159mSynFkSNHoqGh4Zxrez0+Ro8eHYMHD46DBw8W3X/w4MGor68/bX11dXVUV1cX3Tdy5Mhe3VNNTU2fX5RSGcizRQzs+cxWnsxWvgbyfP1ltnM943FKr7/gtKqqKmbMmBHr1q0r3Nfd3R3r1q2Lpqam3n44AKDMlOTbLosWLYoFCxbEVVddFVdffXU8+uijcezYsbjzzjtL8XAAQBkpSXzceuut8bOf/SweeuihaGtriw996EOxZs2a016EWmrV1dXx8MMPn/ZtnYFgIM8WMbDnM1t5Mlv5GsjzletsFemdvCcGAKCX+N0uAEBW4gMAyEp8AABZiQ8AIKsBHR/Lly+PiRMnxtChQ2PmzJnx7LPP9vWW3tYXvvCFqKioKLpdcsklhePHjx+PhQsXxgUXXBDnnXde3HLLLaf9MLd9+/bFvHnzYvjw4TF27Nh44IEH4o033sg9SkREbNy4MW688cZoaGiIioqKWLVqVdHxlFI89NBDMW7cuBg2bFg0NzfHiy++WLTmlVdeifnz50dNTU2MHDky7rrrrjh69GjRmu3bt8e1114bQ4cOjcbGxvjiF79Y6tHOOdsdd9xx2rWcM2dO0Zr+OltLS0t8+MMfjhEjRsTYsWPj5ptvjp07dxat6a3PxfXr18eVV14Z1dXVMWXKlFixYkWfz3b99defdu0++clP9vvZHnvssZg2bVrhh001NTXFd7/73cLxcr1m72S2cr1mZ7Js2bKoqKiI+++/v3BfOV+7s0oD1MqVK1NVVVX62te+ln784x+nu+++O40cOTIdPHiwr7d2Vg8//HC67LLL0oEDBwq3n/3sZ4Xjn/zkJ1NjY2Nat25d2rJlS/rIRz6Sfv3Xf71w/I033kiXX355am5uTs8991z6zne+k0aPHp2WLFnSF+Ok73znO+mP/uiP0re+9a0UEenJJ58sOr5s2bJUW1ubVq1alX70ox+l3/7t306TJk1Kr732WmHNnDlz0vTp09OmTZvSv//7v6cpU6ak22+/vXC8o6Mj1dXVpfnz56cdO3akJ554Ig0bNiz99V//dZ/OtmDBgjRnzpyia/nKK68Uremvs82ePTs9/vjjaceOHWnbtm3pt37rt9L48ePT0aNHC2t643Pxv//7v9Pw4cPTokWL0gsvvJC+/OUvp8GDB6c1a9b06Wy/8Ru/ke6+++6ia9fR0dHvZ/vnf/7ntHr16vRf//VfaefOnelzn/tcGjJkSNqxY0dKqXyv2TuZrVyv2Vs9++yzaeLEiWnatGnpU5/6VOH+cr52ZzNg4+Pqq69OCxcuLHx88uTJ1NDQkFpaWvpwV2/v4YcfTtOnTz/jsfb29jRkyJD0zW9+s3DfT37ykxQRqbW1NaX05hfEQYMGpba2tsKaxx57LNXU1KSurq6S7v1c3voFuru7O9XX16e/+Iu/KNzX3t6eqqur0xNPPJFSSumFF15IEZF+8IMfFNZ897vfTRUVFel///d/U0opfeUrX0nnn39+0Xyf/exn09SpU0s80S+cLT5uuumms/6dcpktpZQOHTqUIiJt2LAhpdR7n4sPPvhguuyyy4oe69Zbb02zZ88u9UgFb50tpTe/kP3y//jfqlxmSyml888/P/3N3/zNgLpmp5yaLaWBcc2OHDmSLr744rR27dqieQbitUsppQH5bZfXX389tm7dGs3NzYX7Bg0aFM3NzdHa2tqHOzu3F198MRoaGmLy5Mkxf/782LdvX0REbN26NU6cOFE00yWXXBLjx48vzNTa2hpXXHFF0Q9zmz17dnR2dsaPf/zjvIOcw549e6Ktra1ontra2pg5c2bRPCNHjoyrrrqqsKa5uTkGDRoUmzdvLqy57rrroqqqqrBm9uzZsXPnzvi///u/TNOc2fr162Ps2LExderUuPfee+Pw4cOFY+U0W0dHR0REjBo1KiJ673OxtbW16Byn1uT8N/rW2U75+7//+xg9enRcfvnlsWTJknj11V/8avNymO3kyZOxcuXKOHbsWDQ1NQ2oa/bW2U4p92u2cOHCmDdv3ml7GEjX7pf1+W+1LYWf//zncfLkydN+ompdXV389Kc/7aNdndvMmTNjxYoVMXXq1Dhw4EAsXbo0rr322tixY0e0tbVFVVXVab90r66uLtra2iIioq2t7YwznzrWn5zaz5n2+8vzjB07tuh4ZWVljBo1qmjNpEmTTjvHqWPnn39+SfZ/LnPmzIlPfOITMWnSpNi9e3d87nOfi7lz50Zra2sMHjy4bGbr7u6O+++/Pz760Y/G5ZdfXnjs3vhcPNuazs7OeO2112LYsGGlGKngTLNFRPzO7/xOTJgwIRoaGmL79u3x2c9+Nnbu3Bnf+ta33nbfp4693ZpSz/b8889HU1NTHD9+PM4777x48skn49JLL41t27aV/TU722wR5X3NIiJWrlwZP/zhD+MHP/jBaccGyr+3txqQ8VGu5s6dW/jztGnTYubMmTFhwoT4x3/8x+yfGLw3t912W+HPV1xxRUybNi0uuuiiWL9+fcyaNasPd9YzCxcujB07dsT3v//9vt5KrzvbbPfcc0/hz1dccUWMGzcuZs2aFbt3746LLroo9zZ7ZOrUqbFt27bo6OiIf/qnf4oFCxbEhg0b+npbveJss1166aVlfc32798fn/rUp2Lt2rUxdOjQvt5ONgPy2y6jR4+OwYMHn/Zq4IMHD0Z9fX0f7arnRo4cGR/84Adj165dUV9fH6+//nq0t7cXrfnlmerr688486lj/cmp/bzdNaqvr49Dhw4VHX/jjTfilVdeKbuZJ0+eHKNHj45du3ZFRHnMdt9998W3v/3teOaZZ+LCCy8s3N9bn4tnW1NTU1Py2D7bbGcyc+bMiIiia9dfZ6uqqoopU6bEjBkzoqWlJaZPnx5/9Vd/NSCu2dlmO5NyumZbt26NQ4cOxZVXXhmVlZVRWVkZGzZsiC996UtRWVkZdXV1ZX/tzmRAxkdVVVXMmDEj1q1bV7ivu7s71q1bV/Q9wv7u6NGjsXv37hg3blzMmDEjhgwZUjTTzp07Y9++fYWZmpqa4vnnny/6orZ27dqoqakpPD3ZX0yaNCnq6+uL5uns7IzNmzcXzdPe3h5bt24trHn66aeju7u78D+Xpqam2LhxY5w4caKwZu3atTF16tQ++5bLmbz00ktx+PDhGDduXET079lSSnHffffFk08+GU8//fRp3/rprc/FpqamonOcWlPKf6Pnmu1Mtm3bFhFRdO3642xn0t3dHV1dXWV9zc7m1GxnUk7XbNasWfH888/Htm3bCrerrroq5s+fX/jzQLt2ETGw32pbXV2dVqxYkV544YV0zz33pJEjRxa9Gri/+fSnP53Wr1+f9uzZk/7jP/4jNTc3p9GjR6dDhw6llN58u9X48ePT008/nbZs2ZKamppSU1NT4e+fervVDTfckLZt25bWrFmTxowZ02dvtT1y5Eh67rnn0nPPPZciIj3yyCPpueeeS//zP/+TUnrzrbYjR45MTz31VNq+fXu66aabzvhW21/7tV9LmzdvTt///vfTxRdfXPR21Pb29lRXV5d+93d/N+3YsSOtXLkyDR8+vORvR3272Y4cOZI+85nPpNbW1rRnz570ve99L1155ZXp4osvTsePH+/3s917772ptrY2rV+/vuiti6+++mphTW98Lp56698DDzyQfvKTn6Tly5eX/K1/55pt165d6U/+5E/Sli1b0p49e9JTTz2VJk+enK677rp+P9vixYvThg0b0p49e9L27dvT4sWLU0VFRfq3f/u3lFL5XrNzzVbO1+xs3vrunXK+dmczYOMjpZS+/OUvp/Hjx6eqqqp09dVXp02bNvX1lt7WrbfemsaNG5eqqqrSBz7wgXTrrbemXbt2FY6/9tpr6fd///fT+eefn4YPH54+/vGPpwMHDhSdY+/evWnu3Llp2LBhafTo0enTn/50OnHiRO5RUkopPfPMMykiTrstWLAgpfTm220///nPp7q6ulRdXZ1mzZqVdu7cWXSOw4cPp9tvvz2dd955qaamJt15553pyJEjRWt+9KMfpWuuuSZVV1enD3zgA2nZsmV9Oturr76abrjhhjRmzJg0ZMiQNGHChHT33XefFr79dbYzzRUR6fHHHy+s6a3PxWeeeSZ96EMfSlVVVWny5MlFj9EXs+3bty9dd911adSoUam6ujpNmTIlPfDAA0U/M6K/zvZ7v/d7acKECamqqiqNGTMmzZo1qxAeKZXvNTvXbOV8zc7mrfFRztfubCpSSinf8ywAwPvdgHzNBwDQf4kPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArP4fSI8JyX7EWjcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view histogram of lengths of sample texts\n",
    "plt.hist(df_sample[\"text_length\"], bins = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f15adc5-ddd6-4d3d-9db4-1765d9248e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on ths historgram, let's use a max length of 1,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fee1f88-a93d-4e3f-8db7-af215802d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tokenizer from DistilBert to tokenize our texts (i.e. turn them into numbers)\n",
    "# the tokeninzer will give each word one number (or more if it uses sub-word tokenization)\n",
    "# after this step, the DistilBert transformer will encode these tokens into vectors when it runs\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70a2ea2b-2ea5-4099-81d2-0ed9cdb4f3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ideology</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The United Nations body responsible for overse...</td>\n",
       "      <td>1</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Criminal Complaint\\n\\nA retired Ohio couple wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“We’re now getting more of these MIS-C kids, b...</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senate Democrats are pushing to reform the fil...</td>\n",
       "      <td>0</td>\n",
       "      <td>2491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One reason Trump supporters believe his lies c...</td>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Glyphosate is a systemic broad-spectrum herbic...</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>100 200 300 400 million Dec. 13 Mar. 21 448 mi...</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>For a long time, we are told, capitalism has b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Myanmar Coup: With Aung San Suu Kyi Detained, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>\"We proudly handed the Biden administration th...</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  ideology  text_length\n",
       "0   The United Nations body responsible for overse...         1          659\n",
       "1   Criminal Complaint\\n\\nA retired Ohio couple wh...         0         1545\n",
       "2   “We’re now getting more of these MIS-C kids, b...         0          304\n",
       "3   Senate Democrats are pushing to reform the fil...         0         2491\n",
       "4   One reason Trump supporters believe his lies c...         0          560\n",
       "..                                                ...       ...          ...\n",
       "88  Glyphosate is a systemic broad-spectrum herbic...         0          299\n",
       "89  100 200 300 400 million Dec. 13 Mar. 21 448 mi...         0          800\n",
       "90  For a long time, we are told, capitalism has b...         0         1195\n",
       "91  Myanmar Coup: With Aung San Suu Kyi Detained, ...         1          861\n",
       "92  \"We proudly handed the Biden administration th...         1          258\n",
       "\n",
       "[93 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset the index in our sample data frame to 0\n",
    "df_sample = df_sample.reset_index(drop=True)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "117b4d4d-d732-4a4e-ac9c-64655785d2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The United Nations body responsible for overseeing Palestinian Refugee Education said recently it had removed violent and anti-Israel content from special materials it published to help Palestinian children study at home during the COVID-19 pandemic. This announcement came after it was challenged by a report from IMPACT-se. But a new investigation questioned that so-called resolution and found the changes didn’t go far enough.\\n\\nThroughout 2020, Palestinian children like many others around the world couldn’t go to school due to COVID-19.\\n\\nDuring that time the United Nations Relief Works Agency or UNRWA, produced and provided the students with material that went way beyond reading, writing and arithmetic.\\n\\n“We found that it contains various violations of UN values, of UNESCO standards and of UNWRA’s own principles. These were educational materials, which were distributed to over 320,000 Palestinian children across the West Bank and Gaza,” said Marcus Sheff, CEO of the Institute for Monitoring Peace and Cultural Tolerance in School Education (IMPACT-se).\\n\\n***Start Your Day with CBN News QuickStart!!! Go here to sign up for QuickStart and other CBN News emails and download the FREE CBN News app to ensure you keep receiving news from a Christian Perspective.***\\n\\nSheff's group examined the material sent to those children and what they found was not good.\\n\\n“The idea that UNWRA, as a UN organization, is distributing material which, calls on students to defend the motherland with blood. Or that glorifies terrorists and directs students to terrorists like Dalal Mughrabi, who is in their materials a role model for young girls, somebody who had murdered 38 people, including 13 children on a civilian bus,” Sheff told CBN News.\\n\\nIMPACT-se produced a report on January 13 that lays out examples found throughout the curriculum.\\n\\nIn mathematics, for instance students are asked to write “The number of martyrs in the first Intifadah” in numerals from written words; a language studies question asks students to find the preposition in a sentence like, “Jihad is one of the doors to paradise;” and in Social Studies, they’re taught “Zionist policy” is responsible for “exhausting Palestinian natural resources.”\\n\\n“It was written by UNWRA’s teachers. Now, UNWRA’s teachers are supposed to be trained in UN values of peace and tolerance,” Sheff said.\\n\\nFollowing the January IMPACT-se report, UNWRA admitted to the problem and assured governments it had been addressed. A follow-up study by the IMPACT team, however, shows that was not the case.\\n\\n“Within these materials, we also found hateful material; material which young people anywhere should not be studying and certainly not being taught by a UN organization,” Sheff said.\\n\\n“So, you know, I think what we see here is that, you know, UNWRA is absolutely part of the problem in relation to the incitement to young people in the region,” he added.\\n\\n***Be sure to sign up for CBN News emails and the CBN News app to ensure you keep receiving news from a Christian Perspective.***\\n\\nIn one social studies exercise, sixth graders are shown a map of the British Mandate called “Palestine” – that does not include any Israel reference and told:\\n\\n“Palestine is the geographical area which extends from the Mediterranean Sea in the west to the River Jordan in the east and from Lebanon and Syria in the north to the Gulf of Aqaba and Egypt in the south…Its location gave it strategic importance, making it coveted by invaders and colonial powers,” it says.\\n\\nAnd there’s another problem.\\n\\n“That is if you’ve distributed printed materials to 320,000 children, then you’ve found that you had been distributing hateful material, how exactly did UNWRA get those materials back? We have no answer to that,” Sheff said.\\n\\nFormer President Trump cut funding to UNRWA because of this kind of incitement in Palestinian schools. President Biden has pledged to resume humanitarian aid to the Palestinians.\\n\\nSheff maintains the world should expect transparency from any United Nations body and until UNRWA can prove otherwise, it should be understood the organization teaches hate.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a9149c2-60bf-415d-bca0-1f4792af2729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1996, 2142, 3741, 2303, 3625, 2005, 19642, 9302, 13141, 2495, 2056, 3728, 2009, 2018, 3718, 6355, 1998, 3424, 1011, 3956, 4180, 2013, 2569, 4475, 2009, 2405, 2000, 2393, 9302, 2336, 2817, 2012, 2188, 2076, 1996, 2522, 17258, 1011, 2539, 6090, 3207, 7712, 1012, 2023, 8874, 2234, 2044, 2009, 2001, 8315, 2011, 1037, 3189, 2013, 4254, 1011, 7367, 1012, 2021, 1037, 2047, 4812, 8781, 2008, 2061, 1011, 2170, 5813, 1998, 2179, 1996, 3431, 2134, 1521, 1056, 2175, 2521, 2438, 1012, 2802, 12609, 1010, 9302, 2336, 2066, 2116, 2500, 2105, 1996, 2088, 2481, 1521, 1056, 2175, 2000, 2082, 2349, 2000, 2522, 17258, 1011, 2539, 1012, 2076, 2008, 2051, 1996, 2142, 3741, 4335, 2573, 4034, 2030, 4895, 2099, 4213, 1010, 2550, 1998, 3024, 1996, 2493, 2007, 3430, 2008, 2253, 2126, 3458, 3752, 1010, 3015, 1998, 20204, 1012, 1523, 2057, 2179, 2008, 2009, 3397, 2536, 13302, 1997, 4895, 5300, 1010, 1997, 12239, 4781, 1998, 1997, 4895, 13088, 2050, 1521, 1055, 2219, 6481, 1012, 2122, 2020, 4547, 4475, 1010, 2029, 2020, 5500, 2000, 2058, 13710, 1010, 2199, 9302, 2336, 2408, 1996, 2225, 2924, 1998, 14474, 1010, 1524, 2056, 6647, 2016, 4246, 1010, 5766, 1997, 1996, 2820, 2005, 8822, 3521, 1998, 3451, 13986, 1999, 2082, 2495, 1006, 4254, 1011, 7367, 1007, 1012, 1008, 1008, 1008, 2707, 2115, 2154, 2007, 21824, 2739, 4248, 14117, 2102, 999, 999, 999, 2175, 2182, 2000, 3696, 2039, 2005, 4248, 14117, 2102, 1998, 2060, 21824, 2739, 22028, 1998, 8816, 1996, 2489, 21824, 2739, 10439, 2000, 5676, 2017, 2562, 4909, 2739, 2013, 1037, 3017, 7339, 1012, 1008, 1008, 1008, 2016, 4246, 1005, 1055, 2177, 8920, 1996, 3430, 2741, 2000, 2216, 2336, 1998, 2054, 2027, 2179, 2001, 2025, 2204, 1012, 1523, 1996, 2801, 2008, 4895, 13088, 2050, 1010, 2004, 1037, 4895, 3029, 1010, 2003, 20083, 3430, 2029, 1010, 4455, 2006, 2493, 2000, 6985, 1996, 2388, 3122, 2007, 2668, 1012, 2030, 2008, 1043, 10626, 14144, 15554, 1998, 23303, 2493, 2000, 15554, 2066, 17488, 2389, 14757, 13492, 5638, 1010, 2040, 2003, 1999, 2037, 4475, 1037, 2535, 2944, 2005, 2402, 3057, 1010, 8307, 2040, 2018, 7129, 4229, 2111, 1010, 2164, 2410, 2336, 2006, 1037, 6831, 3902, 1010, 1524, 2016, 4246, 2409, 21824, 2739, 1012, 4254, 1011, 7367, 2550, 1037, 3189, 2006, 2254, 2410, 2008, 19764, 2041, 4973, 2179, 2802, 1996, 8882, 1012, 1999, 5597, 1010, 2005, 6013, 2493, 2024, 2356, 2000, 4339, 1523, 1996, 2193, 1997, 18945, 1999, 1996, 2034, 20014, 10128, 8447, 2232, 1524, 1999, 16371, 28990, 2015, 2013, 2517, 2616, 1025, 1037, 2653, 2913, 3160, 5176, 2493, 2000, 2424, 1996, 17463, 19234, 1999, 1037, 6251, 2066, 1010, 1523, 24815, 2003, 2028, 1997, 1996, 4303, 2000, 9097, 1025, 1524, 1998, 1999, 2591, 2913, 1010, 2027, 1521, 2128, 4036, 1523, 21379, 3343, 1524, 2003, 3625, 2005, 1523, 15095, 2075, 9302, 3019, 4219, 1012, 1524, 1523, 2009, 2001, 2517, 2011, 4895, 13088, 2050, 1521, 1055, 5089, 1012, 2085, 1010, 4895, 13088, 2050, 1521, 1055, 5089, 2024, 4011, 2000, 2022, 4738, 1999, 4895, 5300, 1997, 3521, 1998, 13986, 1010, 1524, 2016, 4246, 2056, 1012, 2206, 1996, 2254, 4254, 1011, 7367, 3189, 1010, 4895, 13088, 2050, 4914, 2000, 1996, 3291, 1998, 8916, 6867, 2009, 2018, 2042, 8280, 1012, 1037, 3582, 1011, 2039, 2817, 2011, 1996, 4254, 2136, 1010, 2174, 1010, 3065, 2008, 2001, 2025, 1996, 2553, 1012, 1523, 2306, 2122, 4475, 1010, 2057, 2036, 2179, 5223, 3993, 3430, 1025, 3430, 2029, 2402, 2111, 5973, 2323, 2025, 2022, 5702, 1998, 5121, 2025, 2108, 4036, 2011, 1037, 4895, 3029, 1010, 1524, 2016, 4246, 2056, 1012, 1523, 2061, 1010, 2017, 2113, 1010, 1045, 2228, 2054, 2057, 2156, 2182, 2003, 2008, 1010, 2017, 2113, 1010, 4895, 13088, 2050, 2003, 7078, 2112, 1997, 1996, 3291, 1999, 7189, 2000, 1996, 4297, 4221, 3672, 2000, 2402, 2111, 1999, 1996, 2555, 1010, 1524, 2002, 2794, 1012, 1008, 1008, 1008, 2022, 2469, 2000, 3696, 2039, 2005, 21824, 2739, 22028, 1998, 1996, 21824, 2739, 10439, 2000, 5676, 2017, 2562, 4909, 2739, 2013, 1037, 3017, 7339, 1012, 1008, 1008, 1008, 1999, 2028, 2591, 2913, 6912, 1010, 4369, 23256, 2024, 3491, 1037, 4949, 1997, 1996, 2329, 11405, 2170, 1523, 8976, 1524, 1516, 2008, 2515, 2025, 2421, 2151, 3956, 4431, 1998, 2409, 1024, 1523, 8976, 2003, 1996, 10056, 2181, 2029, 8908, 2013, 1996, 7095, 2712, 1999, 1996, 2225, 2000, 1996, 2314, 5207, 1999, 1996, 2264, 1998, 2013, 8341, 1998, 7795, 1999, 1996, 2167, 2000, 1996, 6084, 1997, 1037, 19062, 3676, 1998, 5279, 1999, 1996, 2148, 1529, 2049, 3295, 2435, 2009, 6143, 5197, 1010, 2437, 2009, 28821, 2011, 17347, 1998, 5336, 4204, 1010, 1524, 2009, 2758, 1012, 1998, 2045, 1521, 1055, 2178, 3291, 1012, 1523, 2008, 2003, 2065, 2017, 1521, 2310, 5500, 6267, 4475, 2000, 13710, 1010, 2199, 2336, 1010, 2059, 2017, 1521, 2310, 2179, 2008, 2017, 2018, 2042, 20083, 5223, 3993, 3430, 1010, 2129, 3599, 2106, 4895, 13088, 2050, 2131, 2216, 4475, 2067, 1029, 2057, 2031, 2053, 3437, 2000, 2008, 1010, 1524, 2016, 4246, 2056, 1012, 2280, 2343, 8398, 3013, 4804, 2000, 4895, 2099, 4213, 2138, 1997, 2023, 2785, 1997, 4297, 4221, 3672, 1999, 9302, 2816, 1012, 2343, 7226, 2368, 2038, 16970, 2000, 13746, 11470, 4681, 2000, 1996, 21524, 1012, 2016, 4246, 9319, 1996, 2088, 2323, 5987, 16987, 2013, 2151, 2142, 3741, 2303, 1998, 2127, 4895, 2099, 4213, 2064, 6011, 4728, 1010, 2009, 2323, 2022, 5319, 1996, 3029, 12011, 5223, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example tokenization with max length of 1000\n",
    "# \"truncation\" means that texts longer than max_length will be cut off\n",
    "# \"padding\" means that tokens with zeroes will be added to the end of token outputs to ensure a length of max_length for all tokenized texts\n",
    "\n",
    "max_length = 1000\n",
    "\n",
    "example_tokens = tokenizer(df_sample[\"text\"][0], max_length = max_length, truncation = True, padding = \"max_length\")\n",
    "example_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "548b23fb-f0e3-48b5-ab18-f5a617dc3560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the length of the tokens with an example\n",
    "len(example_tokens[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64a3b300-fbce-40b6-907f-0688c3242d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa5e9668-9774-47c7-850b-fa64d3c81595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a max_len of 50 here to save time making the encodings\n",
    "def construct_tokens(x, tokenizer, max_len = 50, truncation = True, padding = \"max_length\"):\n",
    "    return tokenizer(x, max_length = max_len, truncation = truncation, padding = padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dd368c1-4695-4e20-8ca5-ddf89702963c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1996, 2142, 3741, 2303, 3625, 2005, 19642, 9302, 13141, 2495, 2056, 3728, 2009, 2018, 3718, 6355, 1998, 3424, 1011, 3956, 4180, 2013, 2569, 4475, 2009, 2405, 2000, 2393, 9302, 2336, 2817, 2012, 2188, 2076, 1996, 2522, 17258, 1011, 2539, 6090, 3207, 7712, 1012, 2023, 8874, 2234, 2044, 2009, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_tokens(df_sample[\"text\"][0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d0117-0cfa-4973-a18c-5c306f445c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b51c21a0-571b-4b31-838c-083aaf97639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our x and y from the dataset\n",
    "x = df_sample[\"text\"].tolist()\n",
    "y = df_sample[\"ideology\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f21bdbeb-e90c-45aa-8639-baf7daf7426e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1996, 2142, 3741, 2303, 3625, 2005, 19642, 9302, 13141, 2495, 2056, 3728, 2009, 2018, 3718, 6355, 1998, 3424, 1011, 3956, 4180, 2013, 2569, 4475, 2009, 2405, 2000, 2393, 9302, 2336, 2817, 2012, 2188, 2076, 1996, 2522, 17258, 1011, 2539, 6090, 3207, 7712, 1012, 2023, 8874, 2234, 2044, 2009, 102], [101, 4735, 12087, 1037, 3394, 4058, 3232, 2040, 2587, 1996, 11292, 24018, 2031, 2042, 5338, 2007, 9714, 2005, 9382, 20699, 2005, 2706, 2007, 2060, 2372, 1997, 1996, 2521, 1011, 2157, 8396, 2177, 2000, 4040, 1996, 1057, 1012, 1055, 1012, 9424, 1012, 12834, 6262, 1010, 3438, 1010, 1998, 2014, 3129, 102], [101, 1523, 2057, 1521, 2128, 2085, 2893, 2062, 1997, 2122, 28616, 1011, 1039, 4268, 1010, 2021, 2023, 2051, 1010, 2009, 2074, 3849, 2008, 1037, 3020, 7017, 1997, 2068, 2024, 2428, 11321, 5665, 1010, 1524, 2056, 2852, 1012, 23455, 2139, 11607, 5332, 1010, 2708, 1997, 16514, 7870, 2012, 2336, 1521, 102], [101, 4001, 8037, 2024, 6183, 2000, 5290, 1996, 10882, 29521, 19966, 2121, 1999, 3433, 2000, 2086, 1997, 14254, 8370, 7878, 1517, 2021, 10643, 2123, 1005, 1056, 4025, 15241, 4986, 2055, 1996, 9824, 2044, 9358, 15061, 12411, 1012, 3533, 2158, 17231, 1010, 1040, 1011, 1059, 1012, 12436, 1012, 1010, 5451, 102], [101, 2028, 3114, 8398, 6793, 2903, 2010, 3658, 3310, 2013, 1037, 3937, 2755, 2055, 1996, 4167, 1024, 2009, 3138, 2062, 5177, 3947, 2000, 15454, 2019, 2801, 2004, 6270, 2084, 2000, 5138, 2009, 2004, 2995, 1012, 1999, 2060, 2616, 1010, 2009, 1521, 1055, 6082, 2000, 2903, 2084, 2000, 2025, 1012, 102], [101, 1045, 2572, 2559, 2012, 1037, 2158, 2040, 2003, 2200, 15560, 1012, 2002, 2003, 1037, 8003, 1998, 1037, 2810, 7309, 1998, 2515, 2025, 2215, 2000, 2175, 2000, 1996, 2902, 1012, 2002, 19981, 2015, 2012, 2033, 1998, 2026, 2147, 4256, 2004, 2057, 3233, 2011, 2256, 7683, 2121, 1999, 1996, 20629, 102], [101, 2343, 3235, 3158, 29470, 1010, 1039, 1012, 7313, 1011, 6889, 1006, 3075, 1997, 3519, 1007, 8130, 2039, 2007, 3235, 3158, 29470, 1999, 4027, 1521, 1055, 5239, 1010, 6108, 1998, 5355, 7637, 1996, 2210, 16669, 2015, 4125, 2000, 1996, 3580, 8798, 1010, 2010, 2576, 5442, 3554, 2007, 2198, 1039, 102], [101, 7145, 1010, 1996, 5404, 3293, 2038, 2146, 2042, 1037, 3287, 1011, 6817, 5884, 1012, 2043, 2308, 2020, 2443, 1010, 2009, 2001, 3952, 2004, 3239, 9485, 2030, 1037, 8257, 5080, 1517, 2030, 2119, 1012, 2085, 1010, 1037, 2047, 2002, 3170, 7520, 4748, 2003, 13659, 2000, 4119, 1996, 22807, 1997, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 20996, 18684, 2638, 5637, 2003, 2028, 1997, 1996, 2087, 12807, 1998, 22979, 4898, 1997, 2256, 4245, 1012, 2014, 2573, 102], [101, 4942, 29234, 2000, 1996, 3842, 4942, 29234, 2085, 2005, 2004, 2210, 2004, 1002, 1016, 1037, 3204, 999, 4942, 29234, 2085, 2005, 2004, 2210, 2004, 1002, 1016, 1037, 3204, 999, 2131, 1996, 3842, 1521, 1055, 4882, 17178, 26587, 1012, 1037, 4882, 17886, 1997, 1996, 2190, 1997, 2256, 6325, 1012, 102], [101, 2216, 2040, 2031, 2499, 1999, 1037, 2586, 5814, 3269, 3325, 2178, 2088, 2043, 2027, 2024, 4846, 2012, 1037, 2512, 19496, 2239, 4322, 1012, 2009, 2001, 2037, 2586, 3325, 2008, 12774, 2093, 2280, 2586, 2372, 2000, 10939, 14017, 3567, 2100, 1521, 1055, 2512, 19496, 2239, 18880, 1010, 3146, 1010, 102], [101, 1037, 1523, 3892, 2265, 1524, 6903, 2165, 1037, 2735, 2023, 2733, 2043, 4113, 2198, 6291, 7463, 1037, 3232, 1997, 25007, 3980, 2012, 9733, 1521, 1055, 24969, 1012, 6291, 1998, 3677, 5261, 16443, 3879, 1999, 1996, 10694, 1523, 4931, 8957, 1524, 6903, 2000, 2131, 1996, 7484, 2376, 3353, 2000, 102], [101, 1996, 6236, 2533, 2056, 9317, 2009, 2038, 2366, 4942, 6873, 26474, 2006, 3674, 2822, 3316, 2008, 3073, 2592, 1998, 4806, 2974, 2578, 1999, 1996, 2142, 2163, 2000, 2156, 2065, 2027, 13382, 1037, 2120, 3036, 3891, 1012, 1000, 7211, 2038, 5117, 1999, 6204, 2008, 14969, 2015, 2256, 10660, 3341, 102], [101, 2795, 1997, 8417, 2045, 2003, 1037, 2146, 2381, 1997, 2231, 8830, 1999, 2943, 6089, 1012, 3365, 2943, 21762, 4839, 1999, 1996, 1057, 1012, 1055, 1012, 4171, 3642, 2000, 5326, 2030, 4942, 5332, 4305, 4371, 1996, 2537, 1997, 10036, 1998, 12990, 10725, 2943, 1012, 2070, 1997, 2122, 21762, 2031, 102], [101, 18527, 1010, 2030, 3383, 11414, 2135, 1010, 1996, 1055, 1012, 1040, 1012, 1055, 1012, 4680, 2012, 2029, 13872, 1521, 1055, 4861, 2001, 4233, 2001, 2218, 2012, 2019, 4547, 3409, 1999, 3417, 21899, 1010, 4174, 1010, 2008, 2018, 2042, 13190, 2000, 1996, 2177, 2011, 1996, 2142, 8285, 3667, 1012, 102], [101, 4522, 4200, 2003, 1037, 2828, 1997, 3949, 2008, 4247, 2000, 4982, 1999, 6217, 1012, 2411, 2009, 2950, 13441, 2008, 4995, 1005, 1056, 2047, 1998, 2070, 2111, 2453, 2130, 2655, 2068, 3418, 1012, 2144, 2049, 12149, 1010, 2116, 2031, 2170, 2715, 4200, 1037, 5592, 2013, 2643, 1012, 2823, 1010, 102], [101, 1996, 14420, 2038, 16981, 2048, 3146, 10093, 14545, 25074, 7747, 1002, 14993, 2454, 2005, 2437, 5560, 1015, 4551, 6487, 24755, 12718, 1010, 17067, 2111, 2408, 1996, 2142, 2163, 1012, 1996, 3316, 1517, 2029, 2031, 2363, 1996, 2922, 2986, 1999, 14420, 2381, 1517, 1523, 17800, 11867, 21511, 2098, 1010, 102], [101, 3507, 9733, 7066, 1024, 1045, 1521, 1049, 7568, 2000, 14970, 2008, 2023, 1053, 2509, 1045, 1521, 2222, 6653, 2000, 3237, 3242, 1997, 1996, 9733, 2604, 1998, 5557, 14855, 4757, 2100, 2097, 2468, 5766, 1012, 1999, 1996, 4654, 8586, 3242, 2535, 1010, 1045, 13566, 2000, 3579, 2026, 19320, 1998, 102], [101, 1037, 2047, 3189, 2013, 14955, 18291, 2080, 2023, 2733, 2699, 2000, 8328, 2422, 2006, 1037, 9575, 2116, 14009, 2031, 4384, 1999, 1996, 2317, 2160, 1012, 3580, 2343, 21911, 2050, 5671, 3849, 2000, 2031, 1037, 2062, 2430, 1998, 13155, 2535, 1999, 1996, 2047, 3447, 2084, 2003, 4050, 22891, 2000, 102], [101, 1037, 2688, 2038, 2042, 3140, 2000, 6366, 2049, 6231, 1997, 2280, 2343, 6221, 8398, 2004, 1037, 2765, 1997, 2009, 2108, 11696, 2011, 5731, 1012, 2429, 2000, 3331, 2685, 24443, 1010, 3434, 10722, 11488, 6784, 1005, 1055, 13844, 9316, 1999, 2624, 4980, 1010, 3146, 1010, 2038, 3718, 2049, 8398, 102], [101, 4079, 1004, 24253, 2003, 4640, 1037, 2047, 2336, 1521, 1055, 2338, 2008, 2097, 8439, 1996, 2166, 1997, 2852, 1012, 4938, 6904, 14194, 2072, 2011, 11888, 2989, 2010, 6613, 24615, 1998, 16907, 2010, 2966, 2476, 2000, 2468, 1523, 2637, 1521, 1055, 3460, 1012, 1524, 2852, 1012, 6904, 14194, 2072, 102], [101, 2004, 1996, 3795, 3947, 2000, 3745, 28896, 8039, 2152, 6718, 1010, 2009, 2097, 2145, 2022, 2086, 2077, 25501, 1997, 2111, 2064, 2131, 1996, 2915, 1517, 8701, 2062, 6677, 1998, 1037, 2936, 4990, 2000, 3671, 5666, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 5344, 1999, 2023, 3746, 2024, 1996, 5344, 1997, 2111, 2057, 1521, 2310, 2439, 2000, 2522, 17258, 1011, 2539, 1012, 2089, 2027, 2717, 1999, 3521, 1012, 2130, 2004, 2019, 7861, 15069, 1010, 2009, 1521, 1055, 2524, 2000, 5674, 1996, 2156, 20744, 19006, 1010, 13346, 2791, 1010, 12721, 1010, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2006, 15060, 1045, 2228, 1997, 2026, 7133, 1517, 1037, 5189, 1010, 2785, 1010, 16405, 16989, 18436, 2450, 2040, 28432, 102], [101, 2076, 1996, 8398, 2086, 1010, 1996, 7655, 1000, 6502, 2733, 1000, 8369, 2041, 2004, 1037, 4066, 1997, 2598, 25852, 2154, 1011, 2806, 8595, 4179, 1012, 2054, 2211, 1999, 2238, 2418, 2004, 1037, 3478, 3947, 2011, 1996, 6221, 1005, 1055, 2317, 2160, 1998, 1037, 3951, 4001, 2000, 3579, 2006, 102], [101, 2047, 3290, 3951, 29490, 10337, 2003, 1058, 14147, 2005, 1996, 3728, 15348, 1057, 1012, 1055, 1012, 2160, 2835, 1999, 2014, 2110, 1010, 4129, 3677, 5487, 16694, 2006, 23466, 2595, 2213, 1521, 1055, 7987, 20175, 8237, 2102, 2739, 5095, 2016, 18754, 2000, 2954, 2005, 1996, 2551, 2465, 1998, 4337, 102], [101, 4841, 2024, 2025, 3403, 2005, 6656, 2000, 3604, 1012, 1996, 6401, 2005, 4295, 2491, 1998, 9740, 2145, 18012, 2114, 3604, 2021, 2951, 2013, 2233, 3065, 4841, 2024, 3500, 2075, 3805, 2007, 3604, 3488, 1012, 3309, 21725, 2015, 3123, 2000, 2037, 3284, 2504, 1999, 2058, 1037, 2095, 1010, 2951, 102], [101, 27754, 2906, 2139, 2744, 2891, 24664, 3406, 8529, 9026, 2019, 2080, 1010, 1051, 19115, 2566, 3207, 2226, 14163, 9956, 2015, 9706, 25463, 7983, 2229, 2053, 2345, 2139, 12609, 1012, 18499, 26354, 2080, 1010, 3653, 22987, 22591, 2015, 19817, 10936, 2121, 14736, 2015, 1015, 1012, 2199, 21877, 24137, 3022, 102], [101, 4199, 1517, 2058, 3263, 2446, 1011, 4092, 3234, 29013, 3843, 1037, 4861, 4465, 21936, 1996, 12111, 1521, 1055, 3522, 8170, 26325, 1996, 13301, 1997, 2168, 1011, 3348, 9209, 1012, 1996, 12111, 3793, 1523, 2003, 7356, 2011, 1037, 15112, 6553, 9218, 1997, 19113, 1998, 5860, 20026, 28184, 2114, 15667, 102], [101, 2129, 2097, 7862, 3342, 6221, 8398, 1029, 2028, 3634, 2086, 3283, 1010, 2637, 3053, 5707, 2046, 1037, 18944, 1012, 1996, 3842, 2001, 5552, 3952, 2011, 2048, 2477, 1024, 1996, 2755, 2008, 1996, 2158, 2040, 6257, 2000, 2022, 2049, 14870, 21237, 1010, 2343, 6221, 8398, 1010, 2001, 2205, 5236, 102], [101, 6221, 8398, 1998, 2010, 9585, 2869, 2031, 2042, 2437, 4447, 1997, 6923, 14303, 9861, 1010, 23294, 8817, 1997, 2111, 2024, 6830, 17800, 1999, 2344, 2000, 19838, 2256, 3864, 1012, 28352, 17791, 1012, 2292, 1521, 1055, 2298, 2012, 1996, 8866, 1998, 2139, 8569, 8950, 2037, 17218, 2320, 1998, 2005, 102], [101, 2343, 3533, 7226, 2368, 1521, 1055, 2933, 2000, 2128, 15222, 8950, 1996, 2142, 2163, 1521, 3921, 2000, 2859, 2071, 2067, 10273, 1010, 2360, 8519, 1010, 2040, 11582, 2008, 2859, 2097, 2298, 2005, 3971, 2000, 2202, 5056, 1997, 1996, 1057, 1012, 1055, 1012, 2096, 7226, 2368, 11333, 18142, 2015, 102], [101, 1996, 2679, 2003, 2006, 999, 2296, 2602, 3049, 5402, 2057, 2360, 2008, 2023, 2003, 1996, 2087, 2590, 2602, 1997, 2256, 3268, 1011, 1011, 1011, 2030, 1999, 2137, 2381, 1012, 2339, 1029, 1517, 2138, 2009, 1521, 1055, 2995, 1012, 2018, 22744, 7207, 2180, 1999, 2355, 1010, 2049, 2825, 2340, 102], [101, 2106, 5205, 6945, 8096, 4019, 1996, 3146, 7071, 2007, 2010, 2564, 1998, 2048, 2402, 2336, 2011, 3909, 2000, 2064, 10841, 2078, 1998, 2681, 2369, 1996, 2155, 3899, 1029, 8096, 2513, 2000, 3146, 2023, 3944, 1010, 2044, 1037, 6398, 2074, 2044, 7090, 9432, 2851, 3936, 1996, 3146, 5205, 2001, 102], [101, 2053, 1010, 2023, 2025, 1037, 2919, 3959, 1012, 2017, 2428, 2106, 2074, 5256, 2039, 1999, 2122, 1000, 2307, 1000, 2142, 2163, 2000, 2424, 2008, 2053, 3043, 2073, 2017, 2444, 1010, 2115, 4071, 1997, 2929, 2003, 2085, 9530, 3367, 20623, 1012, 2025, 2011, 1996, 2510, 1010, 2021, 2011, 1996, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2004, 2057, 4607, 1996, 2345, 7683, 1997, 16080, 2077, 1996, 2602, 1010, 1037, 8068, 2685, 2033, 2000, 1037, 9762, 102], [101, 10439, 9722, 12849, 9153, 3449, 29218, 5886, 3695, 2226, 1010, 2040, 3130, 2170, 3086, 2000, 1996, 3291, 1997, 8040, 3286, 18726, 2006, 6207, 1521, 1055, 16380, 10439, 3573, 1010, 2038, 6406, 1037, 9870, 2114, 6207, 1999, 2662, 16723, 1996, 2194, 1997, 18077, 2075, 2049, 15404, 2373, 2058, 18726, 102], [101, 18346, 1010, 2605, 1006, 9706, 1007, 1516, 25209, 2050, 1521, 1055, 2413, 7506, 1998, 2195, 1997, 2049, 12706, 2024, 2275, 2000, 2175, 2006, 3979, 6928, 2058, 13519, 2008, 2027, 17800, 11867, 6340, 2006, 5126, 1998, 6304, 1012, 3119, 9209, 2988, 1996, 7390, 1998, 2188, 5350, 2194, 2000, 2413, 102], [101, 2343, 6221, 8398, 14429, 2135, 2170, 2110, 4905, 2236, 8226, 7148, 18940, 17668, 4590, 1010, 2022, 15172, 2032, 2005, 2010, 3279, 1997, 4108, 1998, 9694, 2002, 1000, 2424, 1000, 2032, 2340, 1010, 2199, 2062, 4494, 1999, 2054, 2070, 3423, 8519, 2360, 2001, 1037, 4735, 2552, 1012, 2021, 2625, 102], [101, 1999, 2048, 3025, 8466, 1010, 3021, 9587, 10532, 2015, 1010, 4988, 2585, 8512, 2100, 1010, 1998, 2500, 2956, 1999, 1996, 2047, 4516, 1010, 22889, 4710, 1996, 5202, 1010, 4541, 2129, 14926, 2386, 4063, 2075, 2038, 6964, 2499, 1998, 2054, 2904, 1999, 2230, 1012, 3951, 22680, 2109, 2417, 2923, 102], [101, 1996, 2739, 17848, 4803, 24304, 2015, 2862, 2097, 2079, 2062, 2084, 23216, 2115, 2568, 1012, 2122, 9631, 2089, 4119, 2115, 9029, 1010, 5041, 2368, 2115, 15251, 1010, 4654, 17847, 2115, 12731, 9488, 24279, 1010, 2030, 21255, 2115, 9647, 1012, 2122, 2808, 2089, 2025, 9352, 3711, 2006, 1996, 2880, 102], [101, 18856, 24237, 2015, 1997, 4907, 9247, 8496, 2031, 10538, 2039, 1999, 2115, 4220, 1010, 2061, 2017, 3362, 2005, 1037, 5835, 1997, 2461, 6279, 1010, 1996, 2759, 17901, 6359, 1012, 2009, 2003, 2124, 2005, 2108, 2200, 4621, 1010, 2021, 2049, 2364, 21774, 1010, 1043, 2135, 8458, 8820, 2618, 1010, 102], [101, 2280, 2413, 2343, 9473, 18906, 3683, 9096, 8480, 2012, 1996, 20747, 6928, 1010, 2233, 1015, 1010, 25682, 1999, 3000, 1012, 1996, 14392, 2003, 3517, 1999, 1037, 8637, 7897, 1998, 3747, 1011, 21877, 21814, 3979, 2008, 2038, 2404, 2413, 2280, 2343, 9473, 18906, 3683, 9096, 2012, 3891, 1997, 1037, 102], [101, 1996, 2343, 1521, 1055, 4740, 2000, 18793, 1999, 2019, 7552, 4812, 2071, 3815, 2000, 27208, 1997, 3425, 2030, 2060, 4735, 13302, 1010, 3423, 8519, 2056, 1010, 2295, 2027, 14046, 2098, 1037, 2553, 2071, 2022, 3697, 2000, 6011, 1012, 4748, 4748, 3187, 1997, 2110, 8226, 7148, 18940, 17668, 4590, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2023, 2466, 2001, 2761, 2405, 2011, 6151, 17007, 1998, 2003, 22296, 2182, 2004, 2112, 1997, 1996, 4785, 4624, 5792, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 1996, 2254, 1020, 2886, 2006, 1996, 9424, 2001, 1010, 2005, 2116, 4841, 1010, 2019, 4895, 15222, 25804, 3468, 1998, 102], [101, 2023, 2466, 2001, 2761, 2405, 2011, 24665, 2923, 1998, 2003, 22296, 2182, 2004, 2112, 1997, 1996, 4785, 4624, 5792, 1012, 2041, 1997, 1996, 3420, 1997, 2026, 3239, 1010, 1045, 3236, 1037, 12185, 1997, 4714, 1017, 1011, 2095, 1011, 2214, 8038, 4313, 19021, 19585, 2080, 24770, 2067, 1998, 5743, 102], [101, 2616, 18953, 2011, 1996, 6075, 2008, 2343, 8398, 2038, 2109, 2068, 1999, 1056, 28394, 3215, 2076, 2010, 2744, 1999, 2436, 1010, 2429, 2000, 1996, 8398, 10474, 8756, 1012, 2899, 1517, 2343, 6221, 8398, 2985, 2172, 1997, 1996, 12609, 4883, 3049, 22604, 2008, 2002, 2071, 2069, 4558, 2065, 1996, 102], [101, 16836, 2013, 2430, 2637, 1998, 2037, 2336, 3422, 2019, 6912, 4146, 2011, 2372, 1997, 1996, 2149, 3675, 6477, 2012, 1996, 2624, 1061, 5332, 22196, 5153, 1999, 2624, 5277, 1012, 2149, 3675, 4584, 2218, 2062, 2084, 1019, 1010, 2199, 14477, 21408, 8737, 7088, 2098, 11560, 2336, 1999, 2037, 9968, 102], [101, 4831, 4557, 2741, 1037, 23921, 2000, 1996, 3059, 2343, 9857, 14026, 2010, 14038, 2058, 1996, 10102, 6928, 1997, 1996, 2406, 1521, 1055, 6059, 2000, 1996, 3537, 3072, 1997, 1996, 9030, 1012, 1523, 2009, 2001, 2007, 14038, 2008, 1045, 4342, 1997, 1996, 13800, 2886, 1999, 1996, 3537, 3072, 1997, 102], [101, 2004, 1996, 1044, 18863, 2105, 2512, 1011, 15289, 3468, 19204, 1006, 1050, 6199, 1007, 2396, 4247, 2000, 4982, 1010, 2116, 2024, 9436, 2594, 18639, 2290, 1996, 13675, 10936, 2063, 2164, 2329, 3364, 1998, 9971, 2198, 18856, 10285, 2063, 2040, 2003, 2085, 4855, 2010, 2219, 5059, 1997, 1996, 6613, 102], [101, 2023, 2733, 3677, 19379, 14634, 26234, 7566, 2000, 5660, 8654, 3166, 6423, 10722, 2869, 10222, 2055, 2014, 2476, 2004, 1037, 2658, 2188, 5660, 1998, 2014, 2047, 2338, 3432, 6423, 1024, 7287, 3733, 19328, 2005, 7965, 7216, 2833, 1012, 1999, 1996, 4357, 1010, 6423, 7607, 2339, 2016, 2001, 4567, 102], [101, 2043, 1996, 24265, 2015, 2234, 2091, 2114, 2280, 8398, 3049, 3472, 2703, 24951, 13028, 2703, 2198, 24951, 13028, 21572, 3366, 12690, 5668, 4530, 3947, 2000, 15126, 2093, 24951, 13028, 5144, 2044, 8398, 14933, 8495, 4107, 1014, 2243, 10377, 2005, 2845, 3275, 11382, 17960, 8238, 2047, 2259, 2457, 3513, 102], [101, 16360, 1012, 3958, 5207, 2508, 1006, 3958, 1007, 3817, 5207, 2890, 14289, 16558, 5555, 3619, 2655, 2005, 4994, 2006, 3675, 12058, 1005, 17542, 3226, 1005, 2003, 2074, 2489, 4613, 3173, 2500, 26771, 11721, 26327, 1010, 5207, 3198, 2160, 14814, 2000, 15113, 9530, 8043, 22879, 5668, 19801, 1010, 8951, 102], [101, 1006, 6646, 2581, 2581, 2620, 1013, 2131, 3723, 4871, 1007, 2006, 2010, 3167, 9927, 1010, 2934, 2726, 3044, 1997, 1996, 2118, 1997, 2624, 5277, 2375, 2082, 2626, 1037, 2695, 2008, 2001, 9249, 4187, 1997, 2822, 2231, 6043, 1012, 3859, 6920, 1010, 1996, 3834, 11240, 5496, 2032, 1997, 5636, 102], [101, 2057, 2064, 2644, 3403, 2005, 1996, 2502, 6543, 5325, 1010, 1996, 4937, 6305, 2135, 6491, 2594, 6092, 12554, 1010, 1996, 25312, 19159, 1997, 7072, 2008, 2116, 2031, 2146, 8615, 1999, 1996, 2142, 2163, 1997, 2637, 1012, 1996, 5325, 2003, 2525, 2182, 1010, 1998, 2057, 1005, 2310, 2042, 2542, 102], [101, 1045, 1521, 1040, 2196, 2657, 1997, 1996, 20907, 2265, 18353, 4890, 1004, 4108, 2127, 3041, 2023, 3204, 1010, 2043, 4202, 9170, 7549, 2009, 2005, 1037, 1523, 13971, 1010, 6171, 3348, 2923, 1524, 8257, 1999, 1037, 1056, 28394, 2102, 2000, 2014, 6070, 2454, 1011, 4606, 8771, 1012, 2076, 1996, 102], [101, 1037, 12720, 2610, 5290, 3021, 2979, 2011, 2160, 8037, 2247, 2283, 3210, 2071, 2404, 2610, 3738, 1999, 4795, 8146, 1010, 1998, 2031, 1037, 4997, 4254, 2006, 14357, 4073, 1010, 2975, 4279, 2008, 2024, 21834, 2094, 2007, 4126, 2130, 2062, 8211, 1012, 3145, 3787, 1997, 1996, 1000, 2577, 12305, 102], [101, 1037, 28475, 2007, 1996, 2004, 6494, 10431, 19281, 1521, 1055, 21887, 23350, 17404, 1999, 4068, 1010, 2762, 1010, 2233, 2385, 1010, 25682, 1012, 1006, 24181, 7003, 2818, 3489, 1013, 26665, 1007, 2651, 2006, 1996, 10195, 1010, 4138, 1010, 4918, 1010, 20789, 1010, 1998, 3958, 6848, 1996, 6745, 2039, 102], [101, 1996, 2034, 2733, 1997, 2254, 2001, 4417, 2011, 2048, 3278, 2824, 1999, 2149, 4331, 1024, 1996, 2448, 1011, 2125, 2602, 9248, 1999, 4108, 1997, 8037, 12551, 11582, 7432, 1998, 2198, 9808, 6499, 4246, 1010, 3228, 1996, 2283, 2491, 1997, 1996, 2149, 4001, 1025, 1998, 1996, 5274, 1997, 1996, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2045, 2024, 2116, 10487, 2005, 3183, 3153, 2003, 2037, 9518, 1998, 2379, 1011, 13048, 6693, 1010, 2040, 3345, 2005, 102], [101, 1996, 2047, 3795, 3189, 4455, 1010, 2426, 2500, 1010, 2005, 5509, 4073, 2000, 4769, 4803, 2300, 6911, 1998, 5335, 1996, 8122, 1997, 2300, 2224, 1999, 5237, 1998, 3068, 1010, 1998, 2009, 22106, 4506, 1999, 2093, 2752, 1024, 2034, 1010, 12067, 2111, 2000, 15581, 2000, 1996, 14670, 1997, 4785, 102], [101, 8003, 3743, 4988, 9152, 22842, 6216, 6769, 2038, 2081, 2381, 2012, 6568, 2011, 3352, 1996, 2034, 2304, 2450, 2000, 3677, 1523, 4024, 3892, 1012, 1524, 6769, 4354, 2014, 2034, 2792, 2006, 9432, 4077, 4901, 26551, 1010, 2040, 2003, 2036, 2304, 1012, 2016, 2003, 1996, 2034, 2450, 1997, 3609, 102], [101, 2047, 2259, 3099, 4080, 12731, 19506, 8847, 2076, 1037, 3679, 27918, 2206, 1996, 8293, 1997, 1996, 21887, 23350, 1999, 2047, 2259, 2103, 1010, 2251, 2410, 1010, 12609, 1012, 1006, 3505, 16562, 2099, 1013, 26665, 1007, 2651, 2006, 1996, 12584, 3189, 1010, 5557, 1998, 4138, 6848, 1996, 2047, 2259, 102], [101, 1996, 18297, 2187, 1996, 4112, 1998, 1037, 8343, 5303, 2044, 7493, 3674, 7171, 1012, 1011, 1011, 1037, 4108, 6458, 1005, 1055, 4112, 2003, 18836, 2005, 2010, 2166, 2044, 25620, 2543, 2007, 1037, 8343, 2096, 2006, 1037, 7574, 2655, 2012, 1037, 2797, 5039, 1999, 3541, 19817, 7140, 2361, 2221, 102], [101, 1999, 4507, 1010, 4004, 2015, 2024, 6524, 2641, 2317, 1010, 1998, 1996, 2944, 1011, 7162, 10661, 14485, 2015, 1996, 6565, 5966, 2426, 4004, 1011, 4841, 1012, 2054, 1521, 1055, 2062, 1010, 1996, 10661, 3271, 2000, 12919, 2637, 1521, 1055, 2317, 4314, 2344, 1010, 2029, 4447, 2000, 27329, 8906, 102], [101, 1037, 2177, 1997, 16836, 2770, 2046, 1996, 4534, 1997, 2474, 23417, 7153, 2019, 4357, 2090, 7987, 20175, 8237, 2102, 3146, 1998, 1057, 1012, 1055, 1012, 12295, 9090, 6060, 1006, 1054, 1011, 19067, 1007, 2040, 2001, 5873, 1996, 2181, 1012, 2076, 2019, 4357, 2012, 2305, 2379, 1996, 5085, 1997, 102], [101, 2585, 24185, 22895, 10464, 21355, 1521, 1055, 10768, 2099, 15338, 14842, 2001, 16021, 13699, 25236, 2013, 2010, 2969, 1011, 23138, 14982, 2015, 1012, 9982, 2011, 2585, 24185, 22895, 10464, 21355, 1998, 3419, 6031, 1013, 14571, 1996, 3776, 1997, 2585, 24185, 22895, 10464, 21355, 1998, 1052, 1012, 1052, 1012, 102], [101, 1996, 26721, 16069, 2140, 7982, 2090, 1996, 2047, 2259, 2695, 1521, 1055, 2061, 13492, 2497, 6289, 7849, 2072, 1998, 2120, 3319, 1521, 1055, 2585, 2413, 4247, 2000, 4895, 10371, 1012, 2070, 2089, 4687, 2043, 2009, 2097, 2203, 1517, 2021, 2045, 2003, 2053, 2062, 2590, 7789, 5981, 1999, 2035, 102], [101, 3883, 12834, 2821, 2081, 1037, 4474, 3311, 2012, 1037, 1523, 2644, 4004, 5223, 1524, 6186, 1999, 6278, 1010, 3552, 1010, 2006, 5095, 1010, 2073, 2016, 2419, 1037, 16883, 4013, 27640, 1010, 1523, 1045, 2572, 7098, 2000, 2022, 4004, 1012, 1524, 1523, 6278, 1010, 1045, 2572, 2061, 3407, 1998, 102], [101, 2064, 2017, 6510, 1999, 1037, 2261, 14189, 2000, 2393, 4636, 2388, 3557, 1005, 15025, 8083, 1029, 2057, 1005, 2128, 1037, 14495, 1006, 2061, 2009, 1005, 1055, 4171, 1011, 2139, 8566, 6593, 7028, 1007, 1010, 1998, 8068, 2490, 3084, 2039, 2055, 2048, 1011, 12263, 1997, 2256, 5166, 1012, 2057, 102], [101, 6074, 1999, 8398, 1005, 1055, 6987, 2031, 2042, 2356, 2055, 26811, 2000, 2225, 5340, 3509, 1012, 2004, 2163, 4088, 2000, 8292, 28228, 12031, 2602, 3463, 2008, 7744, 1037, 3377, 2005, 2343, 1011, 11322, 3533, 7226, 2368, 1010, 2130, 2295, 6221, 8398, 8440, 1005, 1056, 15848, 2045, 2003, 2028, 102], [101, 11091, 7047, 10878, 2869, 1010, 1046, 1012, 1041, 1012, 1006, 12609, 1007, 1012, 1523, 2045, 1521, 1055, 2467, 1037, 15111, 2105, 2043, 2017, 2123, 1521, 1056, 2215, 2028, 1524, 1024, 2054, 7825, 2064, 6570, 1996, 2277, 2865, 3698, 1012, 1999, 2585, 6469, 7825, 1998, 4676, 1024, 8927, 2006, 102], [101, 3000, 1006, 9706, 1007, 1517, 3901, 1997, 3000, 1998, 2195, 2060, 4655, 1997, 2605, 2985, 2037, 2034, 5353, 2104, 1037, 3132, 3204, 10052, 5843, 7698, 1012, 2096, 1996, 2413, 2231, 7278, 1996, 3513, 2052, 2022, 2625, 9384, 2084, 1999, 1996, 2627, 1010, 1996, 5761, 2031, 2042, 6367, 2004, 102], [101, 1037, 19633, 5271, 7245, 1999, 1996, 29461, 3022, 9496, 2229, 3006, 2001, 22464, 2011, 1037, 5294, 16388, 2013, 2759, 14279, 1010, 21106, 2011, 6381, 3012, 5936, 2008, 2071, 1999, 29301, 2062, 3255, 1999, 2746, 2420, 1012, 1996, 16388, 3047, 2012, 1037, 2051, 2043, 13066, 2020, 2525, 5191, 2055, 102], [101, 2320, 1996, 2522, 17258, 1011, 2539, 4335, 3021, 4641, 2000, 1996, 4001, 1010, 1996, 1002, 2321, 6263, 11897, 2097, 2227, 2048, 2502, 5852, 1024, 2028, 24508, 1010, 1998, 2028, 2576, 1012, 2006, 1996, 24508, 2392, 1010, 8037, 2342, 2000, 8054, 1996, 4001, 3323, 12199, 1517, 2019, 1999, 1011, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 4098, 20997, 2052, 2031, 2357, 5989, 7483, 1012, 2044, 2197, 2733, 1521, 1055, 9252, 9424, 29216, 1010, 2043, 1996, 102], [101, 1996, 2190, 24095, 6209, 2283, 2003, 6230, 5095, 1998, 2057, 2215, 2017, 2000, 3693, 2149, 999, 2004, 12609, 3310, 2000, 1037, 2485, 1010, 3693, 16092, 6031, 1998, 9606, 5671, 1011, 6890, 1010, 2522, 1011, 6184, 1997, 1996, 2291, 4638, 16110, 2013, 1996, 3842, 1010, 2005, 1037, 2200, 2569, 102], [101, 17047, 4456, 20543, 2089, 12996, 3945, 2000, 5672, 2088, 1005, 1055, 2087, 2691, 17901, 6359, 1000, 2561, 3571, 1998, 5213, 1012, 1000, 2008, 1005, 1055, 2129, 4080, 14161, 14643, 1010, 1037, 17901, 7155, 2012, 1996, 2118, 1997, 10622, 1999, 13679, 9856, 1010, 5577, 1996, 4668, 1997, 6617, 2000, 102], [101, 4841, 2040, 3427, 4419, 2739, 1005, 4422, 11382, 13728, 13775, 2063, 4357, 2343, 6221, 8398, 2024, 23558, 2012, 1996, 1000, 4419, 1004, 2814, 1000, 2522, 1011, 3677, 1005, 1055, 1000, 7676, 2989, 1000, 1998, 1000, 4352, 1000, 1996, 2343, 2000, 11867, 7974, 1000, 4682, 2044, 4682, 2044, 4682, 102], [101, 1996, 16266, 3597, 4502, 2221, 2604, 1997, 22565, 2442, 2735, 2058, 17069, 1998, 21628, 9513, 6681, 2000, 4001, 3951, 4177, 2061, 2027, 2064, 6204, 2019, 15727, 1997, 1996, 12609, 2236, 2602, 1010, 1037, 3648, 5451, 1012, 16266, 3597, 4502, 2221, 6020, 2457, 3648, 10805, 2726, 2239, 5451, 2006, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 4108, 3537, 12411, 1012, 6798, 11582, 7432, 21052, 6367, 2110, 10643, 2006, 4465, 2851, 1010, 3038, 2009, 2001, 1037, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2043, 7713, 1011, 1999, 1011, 2173, 4449, 7260, 1996, 2406, 1999, 2233, 1010, 6683, 9581, 8038, 29045, 2050, 1521, 102], [101, 4108, 10643, 2031, 3107, 2664, 2178, 27284, 3021, 2008, 2052, 21040, 2689, 1996, 2110, 1005, 1055, 6830, 4277, 1010, 2023, 2051, 17903, 1037, 2048, 1011, 3931, 6378, 2055, 9962, 4402, 5097, 2046, 1037, 6109, 1011, 3931, 27284, 2207, 2019, 3178, 2077, 1037, 2837, 3116, 1012, 2004, 2009, 2979, 102], [101, 1037, 2662, 2450, 2001, 2022, 9250, 2011, 2019, 7904, 2012, 1037, 5806, 3806, 2276, 2023, 2733, 2005, 4092, 3009, 2000, 2178, 2450, 2040, 3047, 2000, 2022, 2551, 2045, 1012, 2334, 2739, 2276, 5925, 1021, 4311, 2008, 2624, 3533, 6319, 24665, 8586, 3148, 17866, 2253, 2046, 1037, 2334, 5806, 102], [101, 1996, 2739, 17848, 4803, 24304, 2015, 2862, 2097, 2079, 2062, 2084, 23216, 2115, 2568, 1012, 2122, 9631, 2089, 4119, 2115, 9029, 1010, 5041, 2368, 2115, 15251, 1010, 4654, 17847, 2115, 12731, 9488, 24279, 1010, 2030, 21255, 2115, 9647, 1012, 2122, 2808, 2089, 2025, 9352, 3711, 2006, 1996, 2880, 102], [101, 1996, 2034, 2792, 1997, 4363, 2039, 2007, 1996, 10556, 13639, 6182, 6962, 1010, 2029, 4836, 2006, 13323, 1012, 2403, 1010, 2289, 1010, 3216, 2074, 2104, 2570, 2781, 2302, 12698, 1012, 2009, 4269, 2007, 1996, 2155, 5378, 6805, 1011, 2946, 4955, 1997, 3209, 1006, 1047, 7317, 8913, 1024, 1523, 102], [101, 3462, 2150, 1037, 3595, 2653, 2005, 19050, 7179, 1010, 1998, 2009, 4247, 2000, 5050, 2304, 12969, 2646, 7931, 1012, 2004, 2019, 6388, 6254, 12199, 1010, 1045, 24964, 2863, 11528, 2063, 1996, 3853, 1997, 8264, 2005, 2304, 2308, 1012, 2045, 2024, 3441, 1996, 10748, 2031, 3950, 1010, 1998, 1045, 102], [101, 1043, 2135, 8458, 8820, 2618, 2003, 1037, 22575, 5041, 1011, 8674, 12810, 21752, 2008, 2003, 2011, 2085, 1996, 2087, 8077, 2109, 12810, 21752, 1999, 1996, 2088, 1998, 2038, 2042, 1996, 3120, 2005, 1037, 2145, 9685, 6704, 2055, 2049, 17631, 3896, 2006, 2529, 2740, 1998, 1996, 4044, 1012, 1996, 102], [101, 2531, 3263, 3998, 4278, 2454, 11703, 1012, 2410, 9388, 1012, 2538, 4008, 2620, 2454, 2561, 21656, 8564, 2062, 2084, 4008, 2620, 2454, 17404, 21656, 2031, 2042, 8564, 4969, 1010, 5020, 2000, 1019, 1012, 1022, 21656, 2005, 2296, 2531, 2111, 1012, 2045, 2003, 2525, 1037, 9762, 6578, 2090, 12436, 102], [101, 2005, 1037, 2146, 2051, 1010, 2057, 2024, 2409, 1010, 16498, 2038, 2042, 2437, 1996, 2088, 13726, 1012, 9217, 1996, 6355, 9755, 2530, 2163, 4703, 5333, 6917, 1010, 13350, 2031, 5224, 2008, 3521, 2090, 3130, 23689, 6590, 15061, 2594, 3741, 1517, 2087, 5546, 1999, 2885, 1517, 2038, 2042, 4719, 102], [101, 12620, 8648, 1024, 2007, 8740, 3070, 2624, 10514, 2226, 18712, 2072, 14620, 1010, 2510, 3138, 2058, 2231, 4372, 8017, 3351, 2023, 3746, 2000, 24679, 14408, 3258, 5164, 2121, 1013, 9617, 3527, 7630, 4034, 3081, 2131, 3723, 4871, 5164, 2121, 1013, 9617, 3527, 7630, 4034, 3081, 2131, 3723, 4871, 102], [101, 1000, 2057, 18067, 4375, 1996, 7226, 2368, 3447, 1996, 2087, 5851, 3675, 1999, 2381, 1012, 2035, 2027, 2018, 2000, 2079, 2001, 2562, 2023, 5744, 1011, 2770, 2291, 2006, 8285, 8197, 10994, 1012, 2612, 1010, 1999, 1996, 8487, 1997, 1037, 2074, 2261, 3134, 1010, 1996, 7226, 2368, 3447, 2038, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = construct_tokens(x, tokenizer)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742dbf2-c89a-4f52-b727-0c6583e9b8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7977b-5afd-422e-afb4-0f8bda7d4bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "089ebc3e-1267-4e4a-8139-f1838c9dafd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(50,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(50,), dtype=tf.int32, name=None)}, TensorSpec(shape=(), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_tfdataset(tokens, y=None):\n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(tokens),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(tokens))\n",
    "    \n",
    "tfdataset = construct_tfdataset(tokens, y)\n",
    "tfdataset\n",
    "\n",
    "### adding in z to get the index!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ead29e5-6d1f-4481-b611-fe8a0922f859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'input_ids': array([  101,  1996,  2142,  3741,  2303,  3625,  2005, 19642,  9302,\n",
       "          13141,  2495,  2056,  3728,  2009,  2018,  3718,  6355,  1998,\n",
       "           3424,  1011,  3956,  4180,  2013,  2569,  4475,  2009,  2405,\n",
       "           2000,  2393,  9302,  2336,  2817,  2012,  2188,  2076,  1996,\n",
       "           2522, 17258,  1011,  2539,  6090,  3207,  7712,  1012,  2023,\n",
       "           8874,  2234,  2044,  2009,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  4735, 12087,  1037,  3394,  4058,  3232,  2040,  2587,\n",
       "           1996, 11292, 24018,  2031,  2042,  5338,  2007,  9714,  2005,\n",
       "           9382, 20699,  2005,  2706,  2007,  2060,  2372,  1997,  1996,\n",
       "           2521,  1011,  2157,  8396,  2177,  2000,  4040,  1996,  1057,\n",
       "           1012,  1055,  1012,  9424,  1012, 12834,  6262,  1010,  3438,\n",
       "           1010,  1998,  2014,  3129,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1523,  2057,  1521,  2128,  2085,  2893,  2062,  1997,\n",
       "           2122, 28616,  1011,  1039,  4268,  1010,  2021,  2023,  2051,\n",
       "           1010,  2009,  2074,  3849,  2008,  1037,  3020,  7017,  1997,\n",
       "           2068,  2024,  2428, 11321,  5665,  1010,  1524,  2056,  2852,\n",
       "           1012, 23455,  2139, 11607,  5332,  1010,  2708,  1997, 16514,\n",
       "           7870,  2012,  2336,  1521,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  4001,  8037,  2024,  6183,  2000,  5290,  1996, 10882,\n",
       "          29521, 19966,  2121,  1999,  3433,  2000,  2086,  1997, 14254,\n",
       "           8370,  7878,  1517,  2021, 10643,  2123,  1005,  1056,  4025,\n",
       "          15241,  4986,  2055,  1996,  9824,  2044,  9358, 15061, 12411,\n",
       "           1012,  3533,  2158, 17231,  1010,  1040,  1011,  1059,  1012,\n",
       "          12436,  1012,  1010,  5451,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2028,  3114,  8398,  6793,  2903,  2010,  3658,  3310,\n",
       "           2013,  1037,  3937,  2755,  2055,  1996,  4167,  1024,  2009,\n",
       "           3138,  2062,  5177,  3947,  2000, 15454,  2019,  2801,  2004,\n",
       "           6270,  2084,  2000,  5138,  2009,  2004,  2995,  1012,  1999,\n",
       "           2060,  2616,  1010,  2009,  1521,  1055,  6082,  2000,  2903,\n",
       "           2084,  2000,  2025,  1012,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1045,  2572,  2559,  2012,  1037,  2158,  2040,  2003,\n",
       "           2200, 15560,  1012,  2002,  2003,  1037,  8003,  1998,  1037,\n",
       "           2810,  7309,  1998,  2515,  2025,  2215,  2000,  2175,  2000,\n",
       "           1996,  2902,  1012,  2002, 19981,  2015,  2012,  2033,  1998,\n",
       "           2026,  2147,  4256,  2004,  2057,  3233,  2011,  2256,  7683,\n",
       "           2121,  1999,  1996, 20629,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2343,  3235,  3158, 29470,  1010,  1039,  1012,  7313,\n",
       "           1011,  6889,  1006,  3075,  1997,  3519,  1007,  8130,  2039,\n",
       "           2007,  3235,  3158, 29470,  1999,  4027,  1521,  1055,  5239,\n",
       "           1010,  6108,  1998,  5355,  7637,  1996,  2210, 16669,  2015,\n",
       "           4125,  2000,  1996,  3580,  8798,  1010,  2010,  2576,  5442,\n",
       "           3554,  2007,  2198,  1039,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  7145,  1010,  1996,  5404,  3293,  2038,  2146,  2042,\n",
       "           1037,  3287,  1011,  6817,  5884,  1012,  2043,  2308,  2020,\n",
       "           2443,  1010,  2009,  2001,  3952,  2004,  3239,  9485,  2030,\n",
       "           1037,  8257,  5080,  1517,  2030,  2119,  1012,  2085,  1010,\n",
       "           1037,  2047,  2002,  3170,  7520,  4748,  2003, 13659,  2000,\n",
       "           4119,  1996, 22807,  1997,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2292,  2256,  8845,  2393,  2017,  2191,  3168,  1997,\n",
       "           1996,  5005,  1024,  4942, 29234,  2000,  1996,  2388,  3557,\n",
       "           3679, 17178,  1998,  2131,  1037, 28667,  9331,  1997,  2739,\n",
       "           2008,  5609,  1012, 20996, 18684,  2638,  5637,  2003,  2028,\n",
       "           1997,  1996,  2087, 12807,  1998, 22979,  4898,  1997,  2256,\n",
       "           4245,  1012,  2014,  2573,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  4942, 29234,  2000,  1996,  3842,  4942, 29234,  2085,\n",
       "           2005,  2004,  2210,  2004,  1002,  1016,  1037,  3204,   999,\n",
       "           4942, 29234,  2085,  2005,  2004,  2210,  2004,  1002,  1016,\n",
       "           1037,  3204,   999,  2131,  1996,  3842,  1521,  1055,  4882,\n",
       "          17178, 26587,  1012,  1037,  4882, 17886,  1997,  1996,  2190,\n",
       "           1997,  2256,  6325,  1012,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2216,  2040,  2031,  2499,  1999,  1037,  2586,  5814,\n",
       "           3269,  3325,  2178,  2088,  2043,  2027,  2024,  4846,  2012,\n",
       "           1037,  2512, 19496,  2239,  4322,  1012,  2009,  2001,  2037,\n",
       "           2586,  3325,  2008, 12774,  2093,  2280,  2586,  2372,  2000,\n",
       "          10939, 14017,  3567,  2100,  1521,  1055,  2512, 19496,  2239,\n",
       "          18880,  1010,  3146,  1010,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1037,  1523,  3892,  2265,  1524,  6903,  2165,  1037,\n",
       "           2735,  2023,  2733,  2043,  4113,  2198,  6291,  7463,  1037,\n",
       "           3232,  1997, 25007,  3980,  2012,  9733,  1521,  1055, 24969,\n",
       "           1012,  6291,  1998,  3677,  5261, 16443,  3879,  1999,  1996,\n",
       "          10694,  1523,  4931,  8957,  1524,  6903,  2000,  2131,  1996,\n",
       "           7484,  2376,  3353,  2000,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1996,  6236,  2533,  2056,  9317,  2009,  2038,  2366,\n",
       "           4942,  6873, 26474,  2006,  3674,  2822,  3316,  2008,  3073,\n",
       "           2592,  1998,  4806,  2974,  2578,  1999,  1996,  2142,  2163,\n",
       "           2000,  2156,  2065,  2027, 13382,  1037,  2120,  3036,  3891,\n",
       "           1012,  1000,  7211,  2038,  5117,  1999,  6204,  2008, 14969,\n",
       "           2015,  2256, 10660,  3341,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2795,  1997,  8417,  2045,  2003,  1037,  2146,  2381,\n",
       "           1997,  2231,  8830,  1999,  2943,  6089,  1012,  3365,  2943,\n",
       "          21762,  4839,  1999,  1996,  1057,  1012,  1055,  1012,  4171,\n",
       "           3642,  2000,  5326,  2030,  4942,  5332,  4305,  4371,  1996,\n",
       "           2537,  1997, 10036,  1998, 12990, 10725,  2943,  1012,  2070,\n",
       "           1997,  2122, 21762,  2031,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101, 18527,  1010,  2030,  3383, 11414,  2135,  1010,  1996,\n",
       "           1055,  1012,  1040,  1012,  1055,  1012,  4680,  2012,  2029,\n",
       "          13872,  1521,  1055,  4861,  2001,  4233,  2001,  2218,  2012,\n",
       "           2019,  4547,  3409,  1999,  3417, 21899,  1010,  4174,  1010,\n",
       "           2008,  2018,  2042, 13190,  2000,  1996,  2177,  2011,  1996,\n",
       "           2142,  8285,  3667,  1012,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  4522,  4200,  2003,  1037,  2828,  1997,  3949,  2008,\n",
       "           4247,  2000,  4982,  1999,  6217,  1012,  2411,  2009,  2950,\n",
       "          13441,  2008,  4995,  1005,  1056,  2047,  1998,  2070,  2111,\n",
       "           2453,  2130,  2655,  2068,  3418,  1012,  2144,  2049, 12149,\n",
       "           1010,  2116,  2031,  2170,  2715,  4200,  1037,  5592,  2013,\n",
       "           2643,  1012,  2823,  1010,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  1996, 14420,  2038, 16981,  2048,  3146, 10093, 14545,\n",
       "          25074,  7747,  1002, 14993,  2454,  2005,  2437,  5560,  1015,\n",
       "           4551,  6487, 24755, 12718,  1010, 17067,  2111,  2408,  1996,\n",
       "           2142,  2163,  1012,  1996,  3316,  1517,  2029,  2031,  2363,\n",
       "           1996,  2922,  2986,  1999, 14420,  2381,  1517,  1523, 17800,\n",
       "          11867, 21511,  2098,  1010,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  3507,  9733,  7066,  1024,  1045,  1521,  1049,  7568,\n",
       "           2000, 14970,  2008,  2023,  1053,  2509,  1045,  1521,  2222,\n",
       "           6653,  2000,  3237,  3242,  1997,  1996,  9733,  2604,  1998,\n",
       "           5557, 14855,  4757,  2100,  2097,  2468,  5766,  1012,  1999,\n",
       "           1996,  4654,  8586,  3242,  2535,  1010,  1045, 13566,  2000,\n",
       "           3579,  2026, 19320,  1998,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1037,  2047,  3189,  2013, 14955, 18291,  2080,  2023,\n",
       "           2733,  2699,  2000,  8328,  2422,  2006,  1037,  9575,  2116,\n",
       "          14009,  2031,  4384,  1999,  1996,  2317,  2160,  1012,  3580,\n",
       "           2343, 21911,  2050,  5671,  3849,  2000,  2031,  1037,  2062,\n",
       "           2430,  1998, 13155,  2535,  1999,  1996,  2047,  3447,  2084,\n",
       "           2003,  4050, 22891,  2000,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1037,  2688,  2038,  2042,  3140,  2000,  6366,  2049,\n",
       "           6231,  1997,  2280,  2343,  6221,  8398,  2004,  1037,  2765,\n",
       "           1997,  2009,  2108, 11696,  2011,  5731,  1012,  2429,  2000,\n",
       "           3331,  2685, 24443,  1010,  3434, 10722, 11488,  6784,  1005,\n",
       "           1055, 13844,  9316,  1999,  2624,  4980,  1010,  3146,  1010,\n",
       "           2038,  3718,  2049,  8398,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  4079,  1004, 24253,  2003,  4640,  1037,  2047,  2336,\n",
       "           1521,  1055,  2338,  2008,  2097,  8439,  1996,  2166,  1997,\n",
       "           2852,  1012,  4938,  6904, 14194,  2072,  2011, 11888,  2989,\n",
       "           2010,  6613, 24615,  1998, 16907,  2010,  2966,  2476,  2000,\n",
       "           2468,  1523,  2637,  1521,  1055,  3460,  1012,  1524,  2852,\n",
       "           1012,  6904, 14194,  2072,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2004,  1996,  3795,  3947,  2000,  3745, 28896,  8039,\n",
       "           2152,  6718,  1010,  2009,  2097,  2145,  2022,  2086,  2077,\n",
       "          25501,  1997,  2111,  2064,  2131,  1996,  2915,  1517,  8701,\n",
       "           2062,  6677,  1998,  1037,  2936,  4990,  2000,  3671,  5666,\n",
       "           1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1996,  5344,  1999,  2023,  3746,  2024,  1996,  5344,\n",
       "           1997,  2111,  2057,  1521,  2310,  2439,  2000,  2522, 17258,\n",
       "           1011,  2539,  1012,  2089,  2027,  2717,  1999,  3521,  1012,\n",
       "           2130,  2004,  2019,  7861, 15069,  1010,  2009,  1521,  1055,\n",
       "           2524,  2000,  5674,  1996,  2156, 20744, 19006,  1010, 13346,\n",
       "           2791,  1010, 12721,  1010,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2292,  2256,  8845,  2393,  2017,  2191,  3168,  1997,\n",
       "           1996,  5005,  1024,  4942, 29234,  2000,  1996,  2388,  3557,\n",
       "           3679, 17178,  1998,  2131,  1037, 28667,  9331,  1997,  2739,\n",
       "           2008,  5609,  1012,  2006, 15060,  1045,  2228,  1997,  2026,\n",
       "           7133,  1517,  1037,  5189,  1010,  2785,  1010, 16405, 16989,\n",
       "          18436,  2450,  2040, 28432,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2076,  1996,  8398,  2086,  1010,  1996,  7655,  1000,\n",
       "           6502,  2733,  1000,  8369,  2041,  2004,  1037,  4066,  1997,\n",
       "           2598, 25852,  2154,  1011,  2806,  8595,  4179,  1012,  2054,\n",
       "           2211,  1999,  2238,  2418,  2004,  1037,  3478,  3947,  2011,\n",
       "           1996,  6221,  1005,  1055,  2317,  2160,  1998,  1037,  3951,\n",
       "           4001,  2000,  3579,  2006,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2047,  3290,  3951, 29490, 10337,  2003,  1058, 14147,\n",
       "           2005,  1996,  3728, 15348,  1057,  1012,  1055,  1012,  2160,\n",
       "           2835,  1999,  2014,  2110,  1010,  4129,  3677,  5487, 16694,\n",
       "           2006, 23466,  2595,  2213,  1521,  1055,  7987, 20175,  8237,\n",
       "           2102,  2739,  5095,  2016, 18754,  2000,  2954,  2005,  1996,\n",
       "           2551,  2465,  1998,  4337,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  4841,  2024,  2025,  3403,  2005,  6656,  2000,  3604,\n",
       "           1012,  1996,  6401,  2005,  4295,  2491,  1998,  9740,  2145,\n",
       "          18012,  2114,  3604,  2021,  2951,  2013,  2233,  3065,  4841,\n",
       "           2024,  3500,  2075,  3805,  2007,  3604,  3488,  1012,  3309,\n",
       "          21725,  2015,  3123,  2000,  2037,  3284,  2504,  1999,  2058,\n",
       "           1037,  2095,  1010,  2951,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101, 27754,  2906,  2139,  2744,  2891, 24664,  3406,  8529,\n",
       "           9026,  2019,  2080,  1010,  1051, 19115,  2566,  3207,  2226,\n",
       "          14163,  9956,  2015,  9706, 25463,  7983,  2229,  2053,  2345,\n",
       "           2139, 12609,  1012, 18499, 26354,  2080,  1010,  3653, 22987,\n",
       "          22591,  2015, 19817, 10936,  2121, 14736,  2015,  1015,  1012,\n",
       "           2199, 21877, 24137,  3022,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  4199,  1517,  2058,  3263,  2446,  1011,  4092,  3234,\n",
       "          29013,  3843,  1037,  4861,  4465, 21936,  1996, 12111,  1521,\n",
       "           1055,  3522,  8170, 26325,  1996, 13301,  1997,  2168,  1011,\n",
       "           3348,  9209,  1012,  1996, 12111,  3793,  1523,  2003,  7356,\n",
       "           2011,  1037, 15112,  6553,  9218,  1997, 19113,  1998,  5860,\n",
       "          20026, 28184,  2114, 15667,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2129,  2097,  7862,  3342,  6221,  8398,  1029,  2028,\n",
       "           3634,  2086,  3283,  1010,  2637,  3053,  5707,  2046,  1037,\n",
       "          18944,  1012,  1996,  3842,  2001,  5552,  3952,  2011,  2048,\n",
       "           2477,  1024,  1996,  2755,  2008,  1996,  2158,  2040,  6257,\n",
       "           2000,  2022,  2049, 14870, 21237,  1010,  2343,  6221,  8398,\n",
       "           1010,  2001,  2205,  5236,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  6221,  8398,  1998,  2010,  9585,  2869,  2031,  2042,\n",
       "           2437,  4447,  1997,  6923, 14303,  9861,  1010, 23294,  8817,\n",
       "           1997,  2111,  2024,  6830, 17800,  1999,  2344,  2000, 19838,\n",
       "           2256,  3864,  1012, 28352, 17791,  1012,  2292,  1521,  1055,\n",
       "           2298,  2012,  1996,  8866,  1998,  2139,  8569,  8950,  2037,\n",
       "          17218,  2320,  1998,  2005,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2343,  3533,  7226,  2368,  1521,  1055,  2933,  2000,\n",
       "           2128, 15222,  8950,  1996,  2142,  2163,  1521,  3921,  2000,\n",
       "           2859,  2071,  2067, 10273,  1010,  2360,  8519,  1010,  2040,\n",
       "          11582,  2008,  2859,  2097,  2298,  2005,  3971,  2000,  2202,\n",
       "           5056,  1997,  1996,  1057,  1012,  1055,  1012,  2096,  7226,\n",
       "           2368, 11333, 18142,  2015,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  1996,  2679,  2003,  2006,   999,  2296,  2602,  3049,\n",
       "           5402,  2057,  2360,  2008,  2023,  2003,  1996,  2087,  2590,\n",
       "           2602,  1997,  2256,  3268,  1011,  1011,  1011,  2030,  1999,\n",
       "           2137,  2381,  1012,  2339,  1029,  1517,  2138,  2009,  1521,\n",
       "           1055,  2995,  1012,  2018, 22744,  7207,  2180,  1999,  2355,\n",
       "           1010,  2049,  2825,  2340,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2106,  5205,  6945,  8096,  4019,  1996,  3146,  7071,\n",
       "           2007,  2010,  2564,  1998,  2048,  2402,  2336,  2011,  3909,\n",
       "           2000,  2064, 10841,  2078,  1998,  2681,  2369,  1996,  2155,\n",
       "           3899,  1029,  8096,  2513,  2000,  3146,  2023,  3944,  1010,\n",
       "           2044,  1037,  6398,  2074,  2044,  7090,  9432,  2851,  3936,\n",
       "           1996,  3146,  5205,  2001,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2053,  1010,  2023,  2025,  1037,  2919,  3959,  1012,\n",
       "           2017,  2428,  2106,  2074,  5256,  2039,  1999,  2122,  1000,\n",
       "           2307,  1000,  2142,  2163,  2000,  2424,  2008,  2053,  3043,\n",
       "           2073,  2017,  2444,  1010,  2115,  4071,  1997,  2929,  2003,\n",
       "           2085,  9530,  3367, 20623,  1012,  2025,  2011,  1996,  2510,\n",
       "           1010,  2021,  2011,  1996,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2292,  2256,  8845,  2393,  2017,  2191,  3168,  1997,\n",
       "           1996,  5005,  1024,  4942, 29234,  2000,  1996,  2388,  3557,\n",
       "           3679, 17178,  1998,  2131,  1037, 28667,  9331,  1997,  2739,\n",
       "           2008,  5609,  1012,  2004,  2057,  4607,  1996,  2345,  7683,\n",
       "           1997, 16080,  2077,  1996,  2602,  1010,  1037,  8068,  2685,\n",
       "           2033,  2000,  1037,  9762,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101, 10439,  9722, 12849,  9153,  3449, 29218,  5886,  3695,\n",
       "           2226,  1010,  2040,  3130,  2170,  3086,  2000,  1996,  3291,\n",
       "           1997,  8040,  3286, 18726,  2006,  6207,  1521,  1055, 16380,\n",
       "          10439,  3573,  1010,  2038,  6406,  1037,  9870,  2114,  6207,\n",
       "           1999,  2662, 16723,  1996,  2194,  1997, 18077,  2075,  2049,\n",
       "          15404,  2373,  2058, 18726,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101, 18346,  1010,  2605,  1006,  9706,  1007,  1516, 25209,\n",
       "           2050,  1521,  1055,  2413,  7506,  1998,  2195,  1997,  2049,\n",
       "          12706,  2024,  2275,  2000,  2175,  2006,  3979,  6928,  2058,\n",
       "          13519,  2008,  2027, 17800, 11867,  6340,  2006,  5126,  1998,\n",
       "           6304,  1012,  3119,  9209,  2988,  1996,  7390,  1998,  2188,\n",
       "           5350,  2194,  2000,  2413,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2343,  6221,  8398, 14429,  2135,  2170,  2110,  4905,\n",
       "           2236,  8226,  7148, 18940, 17668,  4590,  1010,  2022, 15172,\n",
       "           2032,  2005,  2010,  3279,  1997,  4108,  1998,  9694,  2002,\n",
       "           1000,  2424,  1000,  2032,  2340,  1010,  2199,  2062,  4494,\n",
       "           1999,  2054,  2070,  3423,  8519,  2360,  2001,  1037,  4735,\n",
       "           2552,  1012,  2021,  2625,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1999,  2048,  3025,  8466,  1010,  3021,  9587, 10532,\n",
       "           2015,  1010,  4988,  2585,  8512,  2100,  1010,  1998,  2500,\n",
       "           2956,  1999,  1996,  2047,  4516,  1010, 22889,  4710,  1996,\n",
       "           5202,  1010,  4541,  2129, 14926,  2386,  4063,  2075,  2038,\n",
       "           6964,  2499,  1998,  2054,  2904,  1999,  2230,  1012,  3951,\n",
       "          22680,  2109,  2417,  2923,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1996,  2739, 17848,  4803, 24304,  2015,  2862,  2097,\n",
       "           2079,  2062,  2084, 23216,  2115,  2568,  1012,  2122,  9631,\n",
       "           2089,  4119,  2115,  9029,  1010,  5041,  2368,  2115, 15251,\n",
       "           1010,  4654, 17847,  2115, 12731,  9488, 24279,  1010,  2030,\n",
       "          21255,  2115,  9647,  1012,  2122,  2808,  2089,  2025,  9352,\n",
       "           3711,  2006,  1996,  2880,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101, 18856, 24237,  2015,  1997,  4907,  9247,  8496,  2031,\n",
       "          10538,  2039,  1999,  2115,  4220,  1010,  2061,  2017,  3362,\n",
       "           2005,  1037,  5835,  1997,  2461,  6279,  1010,  1996,  2759,\n",
       "          17901,  6359,  1012,  2009,  2003,  2124,  2005,  2108,  2200,\n",
       "           4621,  1010,  2021,  2049,  2364, 21774,  1010,  1043,  2135,\n",
       "           8458,  8820,  2618,  1010,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2280,  2413,  2343,  9473, 18906,  3683,  9096,  8480,\n",
       "           2012,  1996, 20747,  6928,  1010,  2233,  1015,  1010, 25682,\n",
       "           1999,  3000,  1012,  1996, 14392,  2003,  3517,  1999,  1037,\n",
       "           8637,  7897,  1998,  3747,  1011, 21877, 21814,  3979,  2008,\n",
       "           2038,  2404,  2413,  2280,  2343,  9473, 18906,  3683,  9096,\n",
       "           2012,  3891,  1997,  1037,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1996,  2343,  1521,  1055,  4740,  2000, 18793,  1999,\n",
       "           2019,  7552,  4812,  2071,  3815,  2000, 27208,  1997,  3425,\n",
       "           2030,  2060,  4735, 13302,  1010,  3423,  8519,  2056,  1010,\n",
       "           2295,  2027, 14046,  2098,  1037,  2553,  2071,  2022,  3697,\n",
       "           2000,  6011,  1012,  4748,  4748,  3187,  1997,  2110,  8226,\n",
       "           7148, 18940, 17668,  4590,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2292,  2256,  8845,  2393,  2017,  2191,  3168,  1997,\n",
       "           1996,  5005,  1024,  4942, 29234,  2000,  1996,  2388,  3557,\n",
       "           3679, 17178,  1998,  2131,  1037, 28667,  9331,  1997,  2739,\n",
       "           2008,  5609,  1012,  2023,  2466,  2001,  2761,  2405,  2011,\n",
       "           6151, 17007,  1998,  2003, 22296,  2182,  2004,  2112,  1997,\n",
       "           1996,  4785,  4624,  5792,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2292,  2256,  8845,  2393,  2017,  2191,  3168,  1997,\n",
       "           1996,  5005,  1024,  4942, 29234,  2000,  1996,  2388,  3557,\n",
       "           3679, 17178,  1998,  2131,  1037, 28667,  9331,  1997,  2739,\n",
       "           2008,  5609,  1012,  1996,  2254,  1020,  2886,  2006,  1996,\n",
       "           9424,  2001,  1010,  2005,  2116,  4841,  1010,  2019,  4895,\n",
       "          15222, 25804,  3468,  1998,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2023,  2466,  2001,  2761,  2405,  2011, 24665,  2923,\n",
       "           1998,  2003, 22296,  2182,  2004,  2112,  1997,  1996,  4785,\n",
       "           4624,  5792,  1012,  2041,  1997,  1996,  3420,  1997,  2026,\n",
       "           3239,  1010,  1045,  3236,  1037, 12185,  1997,  4714,  1017,\n",
       "           1011,  2095,  1011,  2214,  8038,  4313, 19021, 19585,  2080,\n",
       "          24770,  2067,  1998,  5743,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2616, 18953,  2011,  1996,  6075,  2008,  2343,  8398,\n",
       "           2038,  2109,  2068,  1999,  1056, 28394,  3215,  2076,  2010,\n",
       "           2744,  1999,  2436,  1010,  2429,  2000,  1996,  8398, 10474,\n",
       "           8756,  1012,  2899,  1517,  2343,  6221,  8398,  2985,  2172,\n",
       "           1997,  1996, 12609,  4883,  3049, 22604,  2008,  2002,  2071,\n",
       "           2069,  4558,  2065,  1996,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101, 16836,  2013,  2430,  2637,  1998,  2037,  2336,  3422,\n",
       "           2019,  6912,  4146,  2011,  2372,  1997,  1996,  2149,  3675,\n",
       "           6477,  2012,  1996,  2624,  1061,  5332, 22196,  5153,  1999,\n",
       "           2624,  5277,  1012,  2149,  3675,  4584,  2218,  2062,  2084,\n",
       "           1019,  1010,  2199, 14477, 21408,  8737,  7088,  2098, 11560,\n",
       "           2336,  1999,  2037,  9968,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  4831,  4557,  2741,  1037, 23921,  2000,  1996,  3059,\n",
       "           2343,  9857, 14026,  2010, 14038,  2058,  1996, 10102,  6928,\n",
       "           1997,  1996,  2406,  1521,  1055,  6059,  2000,  1996,  3537,\n",
       "           3072,  1997,  1996,  9030,  1012,  1523,  2009,  2001,  2007,\n",
       "          14038,  2008,  1045,  4342,  1997,  1996, 13800,  2886,  1999,\n",
       "           1996,  3537,  3072,  1997,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2004,  1996,  1044, 18863,  2105,  2512,  1011, 15289,\n",
       "           3468, 19204,  1006,  1050,  6199,  1007,  2396,  4247,  2000,\n",
       "           4982,  1010,  2116,  2024,  9436,  2594, 18639,  2290,  1996,\n",
       "          13675, 10936,  2063,  2164,  2329,  3364,  1998,  9971,  2198,\n",
       "          18856, 10285,  2063,  2040,  2003,  2085,  4855,  2010,  2219,\n",
       "           5059,  1997,  1996,  6613,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2023,  2733,  3677, 19379, 14634, 26234,  7566,  2000,\n",
       "           5660,  8654,  3166,  6423, 10722,  2869, 10222,  2055,  2014,\n",
       "           2476,  2004,  1037,  2658,  2188,  5660,  1998,  2014,  2047,\n",
       "           2338,  3432,  6423,  1024,  7287,  3733, 19328,  2005,  7965,\n",
       "           7216,  2833,  1012,  1999,  1996,  4357,  1010,  6423,  7607,\n",
       "           2339,  2016,  2001,  4567,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2043,  1996, 24265,  2015,  2234,  2091,  2114,  2280,\n",
       "           8398,  3049,  3472,  2703, 24951, 13028,  2703,  2198, 24951,\n",
       "          13028, 21572,  3366, 12690,  5668,  4530,  3947,  2000, 15126,\n",
       "           2093, 24951, 13028,  5144,  2044,  8398, 14933,  8495,  4107,\n",
       "           1014,  2243, 10377,  2005,  2845,  3275, 11382, 17960,  8238,\n",
       "           2047,  2259,  2457,  3513,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101, 16360,  1012,  3958,  5207,  2508,  1006,  3958,  1007,\n",
       "           3817,  5207,  2890, 14289, 16558,  5555,  3619,  2655,  2005,\n",
       "           4994,  2006,  3675, 12058,  1005, 17542,  3226,  1005,  2003,\n",
       "           2074,  2489,  4613,  3173,  2500, 26771, 11721, 26327,  1010,\n",
       "           5207,  3198,  2160, 14814,  2000, 15113,  9530,  8043, 22879,\n",
       "           5668, 19801,  1010,  8951,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1006,  6646,  2581,  2581,  2620,  1013,  2131,  3723,\n",
       "           4871,  1007,  2006,  2010,  3167,  9927,  1010,  2934,  2726,\n",
       "           3044,  1997,  1996,  2118,  1997,  2624,  5277,  2375,  2082,\n",
       "           2626,  1037,  2695,  2008,  2001,  9249,  4187,  1997,  2822,\n",
       "           2231,  6043,  1012,  3859,  6920,  1010,  1996,  3834, 11240,\n",
       "           5496,  2032,  1997,  5636,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2057,  2064,  2644,  3403,  2005,  1996,  2502,  6543,\n",
       "           5325,  1010,  1996,  4937,  6305,  2135,  6491,  2594,  6092,\n",
       "          12554,  1010,  1996, 25312, 19159,  1997,  7072,  2008,  2116,\n",
       "           2031,  2146,  8615,  1999,  1996,  2142,  2163,  1997,  2637,\n",
       "           1012,  1996,  5325,  2003,  2525,  2182,  1010,  1998,  2057,\n",
       "           1005,  2310,  2042,  2542,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1045,  1521,  1040,  2196,  2657,  1997,  1996, 20907,\n",
       "           2265, 18353,  4890,  1004,  4108,  2127,  3041,  2023,  3204,\n",
       "           1010,  2043,  4202,  9170,  7549,  2009,  2005,  1037,  1523,\n",
       "          13971,  1010,  6171,  3348,  2923,  1524,  8257,  1999,  1037,\n",
       "           1056, 28394,  2102,  2000,  2014,  6070,  2454,  1011,  4606,\n",
       "           8771,  1012,  2076,  1996,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1037, 12720,  2610,  5290,  3021,  2979,  2011,  2160,\n",
       "           8037,  2247,  2283,  3210,  2071,  2404,  2610,  3738,  1999,\n",
       "           4795,  8146,  1010,  1998,  2031,  1037,  4997,  4254,  2006,\n",
       "          14357,  4073,  1010,  2975,  4279,  2008,  2024, 21834,  2094,\n",
       "           2007,  4126,  2130,  2062,  8211,  1012,  3145,  3787,  1997,\n",
       "           1996,  1000,  2577, 12305,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  1037, 28475,  2007,  1996,  2004,  6494, 10431, 19281,\n",
       "           1521,  1055, 21887, 23350, 17404,  1999,  4068,  1010,  2762,\n",
       "           1010,  2233,  2385,  1010, 25682,  1012,  1006, 24181,  7003,\n",
       "           2818,  3489,  1013, 26665,  1007,  2651,  2006,  1996, 10195,\n",
       "           1010,  4138,  1010,  4918,  1010, 20789,  1010,  1998,  3958,\n",
       "           6848,  1996,  6745,  2039,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  1996,  2034,  2733,  1997,  2254,  2001,  4417,  2011,\n",
       "           2048,  3278,  2824,  1999,  2149,  4331,  1024,  1996,  2448,\n",
       "           1011,  2125,  2602,  9248,  1999,  4108,  1997,  8037, 12551,\n",
       "          11582,  7432,  1998,  2198,  9808,  6499,  4246,  1010,  3228,\n",
       "           1996,  2283,  2491,  1997,  1996,  2149,  4001,  1025,  1998,\n",
       "           1996,  5274,  1997,  1996,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2292,  2256,  8845,  2393,  2017,  2191,  3168,  1997,\n",
       "           1996,  5005,  1024,  4942, 29234,  2000,  1996,  2388,  3557,\n",
       "           3679, 17178,  1998,  2131,  1037, 28667,  9331,  1997,  2739,\n",
       "           2008,  5609,  1012,  2045,  2024,  2116, 10487,  2005,  3183,\n",
       "           3153,  2003,  2037,  9518,  1998,  2379,  1011, 13048,  6693,\n",
       "           1010,  2040,  3345,  2005,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1996,  2047,  3795,  3189,  4455,  1010,  2426,  2500,\n",
       "           1010,  2005,  5509,  4073,  2000,  4769,  4803,  2300,  6911,\n",
       "           1998,  5335,  1996,  8122,  1997,  2300,  2224,  1999,  5237,\n",
       "           1998,  3068,  1010,  1998,  2009, 22106,  4506,  1999,  2093,\n",
       "           2752,  1024,  2034,  1010, 12067,  2111,  2000, 15581,  2000,\n",
       "           1996, 14670,  1997,  4785,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  8003,  3743,  4988,  9152, 22842,  6216,  6769,  2038,\n",
       "           2081,  2381,  2012,  6568,  2011,  3352,  1996,  2034,  2304,\n",
       "           2450,  2000,  3677,  1523,  4024,  3892,  1012,  1524,  6769,\n",
       "           4354,  2014,  2034,  2792,  2006,  9432,  4077,  4901, 26551,\n",
       "           1010,  2040,  2003,  2036,  2304,  1012,  2016,  2003,  1996,\n",
       "           2034,  2450,  1997,  3609,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2047,  2259,  3099,  4080, 12731, 19506,  8847,  2076,\n",
       "           1037,  3679, 27918,  2206,  1996,  8293,  1997,  1996, 21887,\n",
       "          23350,  1999,  2047,  2259,  2103,  1010,  2251,  2410,  1010,\n",
       "          12609,  1012,  1006,  3505, 16562,  2099,  1013, 26665,  1007,\n",
       "           2651,  2006,  1996, 12584,  3189,  1010,  5557,  1998,  4138,\n",
       "           6848,  1996,  2047,  2259,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  1996, 18297,  2187,  1996,  4112,  1998,  1037,  8343,\n",
       "           5303,  2044,  7493,  3674,  7171,  1012,  1011,  1011,  1037,\n",
       "           4108,  6458,  1005,  1055,  4112,  2003, 18836,  2005,  2010,\n",
       "           2166,  2044, 25620,  2543,  2007,  1037,  8343,  2096,  2006,\n",
       "           1037,  7574,  2655,  2012,  1037,  2797,  5039,  1999,  3541,\n",
       "          19817,  7140,  2361,  2221,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1999,  4507,  1010,  4004,  2015,  2024,  6524,  2641,\n",
       "           2317,  1010,  1998,  1996,  2944,  1011,  7162, 10661, 14485,\n",
       "           2015,  1996,  6565,  5966,  2426,  4004,  1011,  4841,  1012,\n",
       "           2054,  1521,  1055,  2062,  1010,  1996, 10661,  3271,  2000,\n",
       "          12919,  2637,  1521,  1055,  2317,  4314,  2344,  1010,  2029,\n",
       "           4447,  2000, 27329,  8906,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1037,  2177,  1997, 16836,  2770,  2046,  1996,  4534,\n",
       "           1997,  2474, 23417,  7153,  2019,  4357,  2090,  7987, 20175,\n",
       "           8237,  2102,  3146,  1998,  1057,  1012,  1055,  1012, 12295,\n",
       "           9090,  6060,  1006,  1054,  1011, 19067,  1007,  2040,  2001,\n",
       "           5873,  1996,  2181,  1012,  2076,  2019,  4357,  2012,  2305,\n",
       "           2379,  1996,  5085,  1997,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2585, 24185, 22895, 10464, 21355,  1521,  1055, 10768,\n",
       "           2099, 15338, 14842,  2001, 16021, 13699, 25236,  2013,  2010,\n",
       "           2969,  1011, 23138, 14982,  2015,  1012,  9982,  2011,  2585,\n",
       "          24185, 22895, 10464, 21355,  1998,  3419,  6031,  1013, 14571,\n",
       "           1996,  3776,  1997,  2585, 24185, 22895, 10464, 21355,  1998,\n",
       "           1052,  1012,  1052,  1012,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1996, 26721, 16069,  2140,  7982,  2090,  1996,  2047,\n",
       "           2259,  2695,  1521,  1055,  2061, 13492,  2497,  6289,  7849,\n",
       "           2072,  1998,  2120,  3319,  1521,  1055,  2585,  2413,  4247,\n",
       "           2000,  4895, 10371,  1012,  2070,  2089,  4687,  2043,  2009,\n",
       "           2097,  2203,  1517,  2021,  2045,  2003,  2053,  2062,  2590,\n",
       "           7789,  5981,  1999,  2035,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  3883, 12834,  2821,  2081,  1037,  4474,  3311,  2012,\n",
       "           1037,  1523,  2644,  4004,  5223,  1524,  6186,  1999,  6278,\n",
       "           1010,  3552,  1010,  2006,  5095,  1010,  2073,  2016,  2419,\n",
       "           1037, 16883,  4013, 27640,  1010,  1523,  1045,  2572,  7098,\n",
       "           2000,  2022,  4004,  1012,  1524,  1523,  6278,  1010,  1045,\n",
       "           2572,  2061,  3407,  1998,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2064,  2017,  6510,  1999,  1037,  2261, 14189,  2000,\n",
       "           2393,  4636,  2388,  3557,  1005, 15025,  8083,  1029,  2057,\n",
       "           1005,  2128,  1037, 14495,  1006,  2061,  2009,  1005,  1055,\n",
       "           4171,  1011,  2139,  8566,  6593,  7028,  1007,  1010,  1998,\n",
       "           8068,  2490,  3084,  2039,  2055,  2048,  1011, 12263,  1997,\n",
       "           2256,  5166,  1012,  2057,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  6074,  1999,  8398,  1005,  1055,  6987,  2031,  2042,\n",
       "           2356,  2055, 26811,  2000,  2225,  5340,  3509,  1012,  2004,\n",
       "           2163,  4088,  2000,  8292, 28228, 12031,  2602,  3463,  2008,\n",
       "           7744,  1037,  3377,  2005,  2343,  1011, 11322,  3533,  7226,\n",
       "           2368,  1010,  2130,  2295,  6221,  8398,  8440,  1005,  1056,\n",
       "          15848,  2045,  2003,  2028,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101, 11091,  7047, 10878,  2869,  1010,  1046,  1012,  1041,\n",
       "           1012,  1006, 12609,  1007,  1012,  1523,  2045,  1521,  1055,\n",
       "           2467,  1037, 15111,  2105,  2043,  2017,  2123,  1521,  1056,\n",
       "           2215,  2028,  1524,  1024,  2054,  7825,  2064,  6570,  1996,\n",
       "           2277,  2865,  3698,  1012,  1999,  2585,  6469,  7825,  1998,\n",
       "           4676,  1024,  8927,  2006,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  3000,  1006,  9706,  1007,  1517,  3901,  1997,  3000,\n",
       "           1998,  2195,  2060,  4655,  1997,  2605,  2985,  2037,  2034,\n",
       "           5353,  2104,  1037,  3132,  3204, 10052,  5843,  7698,  1012,\n",
       "           2096,  1996,  2413,  2231,  7278,  1996,  3513,  2052,  2022,\n",
       "           2625,  9384,  2084,  1999,  1996,  2627,  1010,  1996,  5761,\n",
       "           2031,  2042,  6367,  2004,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  1037, 19633,  5271,  7245,  1999,  1996, 29461,  3022,\n",
       "           9496,  2229,  3006,  2001, 22464,  2011,  1037,  5294, 16388,\n",
       "           2013,  2759, 14279,  1010, 21106,  2011,  6381,  3012,  5936,\n",
       "           2008,  2071,  1999, 29301,  2062,  3255,  1999,  2746,  2420,\n",
       "           1012,  1996, 16388,  3047,  2012,  1037,  2051,  2043, 13066,\n",
       "           2020,  2525,  5191,  2055,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  2320,  1996,  2522, 17258,  1011,  2539,  4335,  3021,\n",
       "           4641,  2000,  1996,  4001,  1010,  1996,  1002,  2321,  6263,\n",
       "          11897,  2097,  2227,  2048,  2502,  5852,  1024,  2028, 24508,\n",
       "           1010,  1998,  2028,  2576,  1012,  2006,  1996, 24508,  2392,\n",
       "           1010,  8037,  2342,  2000,  8054,  1996,  4001,  3323, 12199,\n",
       "           1517,  2019,  1999,  1011,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2292,  2256,  8845,  2393,  2017,  2191,  3168,  1997,\n",
       "           1996,  5005,  1024,  4942, 29234,  2000,  1996,  2388,  3557,\n",
       "           3679, 17178,  1998,  2131,  1037, 28667,  9331,  1997,  2739,\n",
       "           2008,  5609,  1012,  4098, 20997,  2052,  2031,  2357,  5989,\n",
       "           7483,  1012,  2044,  2197,  2733,  1521,  1055,  9252,  9424,\n",
       "          29216,  1010,  2043,  1996,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1996,  2190, 24095,  6209,  2283,  2003,  6230,  5095,\n",
       "           1998,  2057,  2215,  2017,  2000,  3693,  2149,   999,  2004,\n",
       "          12609,  3310,  2000,  1037,  2485,  1010,  3693, 16092,  6031,\n",
       "           1998,  9606,  5671,  1011,  6890,  1010,  2522,  1011,  6184,\n",
       "           1997,  1996,  2291,  4638, 16110,  2013,  1996,  3842,  1010,\n",
       "           2005,  1037,  2200,  2569,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101, 17047,  4456, 20543,  2089, 12996,  3945,  2000,  5672,\n",
       "           2088,  1005,  1055,  2087,  2691, 17901,  6359,  1000,  2561,\n",
       "           3571,  1998,  5213,  1012,  1000,  2008,  1005,  1055,  2129,\n",
       "           4080, 14161, 14643,  1010,  1037, 17901,  7155,  2012,  1996,\n",
       "           2118,  1997, 10622,  1999, 13679,  9856,  1010,  5577,  1996,\n",
       "           4668,  1997,  6617,  2000,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  4841,  2040,  3427,  4419,  2739,  1005,  4422, 11382,\n",
       "          13728, 13775,  2063,  4357,  2343,  6221,  8398,  2024, 23558,\n",
       "           2012,  1996,  1000,  4419,  1004,  2814,  1000,  2522,  1011,\n",
       "           3677,  1005,  1055,  1000,  7676,  2989,  1000,  1998,  1000,\n",
       "           4352,  1000,  1996,  2343,  2000, 11867,  7974,  1000,  4682,\n",
       "           2044,  4682,  2044,  4682,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1996, 16266,  3597,  4502,  2221,  2604,  1997, 22565,\n",
       "           2442,  2735,  2058, 17069,  1998, 21628,  9513,  6681,  2000,\n",
       "           4001,  3951,  4177,  2061,  2027,  2064,  6204,  2019, 15727,\n",
       "           1997,  1996, 12609,  2236,  2602,  1010,  1037,  3648,  5451,\n",
       "           1012, 16266,  3597,  4502,  2221,  6020,  2457,  3648, 10805,\n",
       "           2726,  2239,  5451,  2006,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2292,  2256,  8845,  2393,  2017,  2191,  3168,  1997,\n",
       "           1996,  5005,  1024,  4942, 29234,  2000,  1996,  2388,  3557,\n",
       "           3679, 17178,  1998,  2131,  1037, 28667,  9331,  1997,  2739,\n",
       "           2008,  5609,  1012,  4108,  3537, 12411,  1012,  6798, 11582,\n",
       "           7432, 21052,  6367,  2110, 10643,  2006,  4465,  2851,  1010,\n",
       "           3038,  2009,  2001,  1037,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2292,  2256,  8845,  2393,  2017,  2191,  3168,  1997,\n",
       "           1996,  5005,  1024,  4942, 29234,  2000,  1996,  2388,  3557,\n",
       "           3679, 17178,  1998,  2131,  1037, 28667,  9331,  1997,  2739,\n",
       "           2008,  5609,  1012,  2043,  7713,  1011,  1999,  1011,  2173,\n",
       "           4449,  7260,  1996,  2406,  1999,  2233,  1010,  6683,  9581,\n",
       "           8038, 29045,  2050,  1521,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  4108, 10643,  2031,  3107,  2664,  2178, 27284,  3021,\n",
       "           2008,  2052, 21040,  2689,  1996,  2110,  1005,  1055,  6830,\n",
       "           4277,  1010,  2023,  2051, 17903,  1037,  2048,  1011,  3931,\n",
       "           6378,  2055,  9962,  4402,  5097,  2046,  1037,  6109,  1011,\n",
       "           3931, 27284,  2207,  2019,  3178,  2077,  1037,  2837,  3116,\n",
       "           1012,  2004,  2009,  2979,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1037,  2662,  2450,  2001,  2022,  9250,  2011,  2019,\n",
       "           7904,  2012,  1037,  5806,  3806,  2276,  2023,  2733,  2005,\n",
       "           4092,  3009,  2000,  2178,  2450,  2040,  3047,  2000,  2022,\n",
       "           2551,  2045,  1012,  2334,  2739,  2276,  5925,  1021,  4311,\n",
       "           2008,  2624,  3533,  6319, 24665,  8586,  3148, 17866,  2253,\n",
       "           2046,  1037,  2334,  5806,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1996,  2739, 17848,  4803, 24304,  2015,  2862,  2097,\n",
       "           2079,  2062,  2084, 23216,  2115,  2568,  1012,  2122,  9631,\n",
       "           2089,  4119,  2115,  9029,  1010,  5041,  2368,  2115, 15251,\n",
       "           1010,  4654, 17847,  2115, 12731,  9488, 24279,  1010,  2030,\n",
       "          21255,  2115,  9647,  1012,  2122,  2808,  2089,  2025,  9352,\n",
       "           3711,  2006,  1996,  2880,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  1996,  2034,  2792,  1997,  4363,  2039,  2007,  1996,\n",
       "          10556, 13639,  6182,  6962,  1010,  2029,  4836,  2006, 13323,\n",
       "           1012,  2403,  1010,  2289,  1010,  3216,  2074,  2104,  2570,\n",
       "           2781,  2302, 12698,  1012,  2009,  4269,  2007,  1996,  2155,\n",
       "           5378,  6805,  1011,  2946,  4955,  1997,  3209,  1006,  1047,\n",
       "           7317,  8913,  1024,  1523,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  3462,  2150,  1037,  3595,  2653,  2005, 19050,  7179,\n",
       "           1010,  1998,  2009,  4247,  2000,  5050,  2304, 12969,  2646,\n",
       "           7931,  1012,  2004,  2019,  6388,  6254, 12199,  1010,  1045,\n",
       "          24964,  2863, 11528,  2063,  1996,  3853,  1997,  8264,  2005,\n",
       "           2304,  2308,  1012,  2045,  2024,  3441,  1996, 10748,  2031,\n",
       "           3950,  1010,  1998,  1045,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  1043,  2135,  8458,  8820,  2618,  2003,  1037, 22575,\n",
       "           5041,  1011,  8674, 12810, 21752,  2008,  2003,  2011,  2085,\n",
       "           1996,  2087,  8077,  2109, 12810, 21752,  1999,  1996,  2088,\n",
       "           1998,  2038,  2042,  1996,  3120,  2005,  1037,  2145,  9685,\n",
       "           6704,  2055,  2049, 17631,  3896,  2006,  2529,  2740,  1998,\n",
       "           1996,  4044,  1012,  1996,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2531,  3263,  3998,  4278,  2454, 11703,  1012,  2410,\n",
       "           9388,  1012,  2538,  4008,  2620,  2454,  2561, 21656,  8564,\n",
       "           2062,  2084,  4008,  2620,  2454, 17404, 21656,  2031,  2042,\n",
       "           8564,  4969,  1010,  5020,  2000,  1019,  1012,  1022, 21656,\n",
       "           2005,  2296,  2531,  2111,  1012,  2045,  2003,  2525,  1037,\n",
       "           9762,  6578,  2090, 12436,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101,  2005,  1037,  2146,  2051,  1010,  2057,  2024,  2409,\n",
       "           1010, 16498,  2038,  2042,  2437,  1996,  2088, 13726,  1012,\n",
       "           9217,  1996,  6355,  9755,  2530,  2163,  4703,  5333,  6917,\n",
       "           1010, 13350,  2031,  5224,  2008,  3521,  2090,  3130, 23689,\n",
       "           6590, 15061,  2594,  3741,  1517,  2087,  5546,  1999,  2885,\n",
       "           1517,  2038,  2042,  4719,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([  101, 12620,  8648,  1024,  2007,  8740,  3070,  2624, 10514,\n",
       "           2226, 18712,  2072, 14620,  1010,  2510,  3138,  2058,  2231,\n",
       "           4372,  8017,  3351,  2023,  3746,  2000, 24679, 14408,  3258,\n",
       "           5164,  2121,  1013,  9617,  3527,  7630,  4034,  3081,  2131,\n",
       "           3723,  4871,  5164,  2121,  1013,  9617,  3527,  7630,  4034,\n",
       "           3081,  2131,  3723,  4871,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1),\n",
       " ({'input_ids': array([  101,  1000,  2057, 18067,  4375,  1996,  7226,  2368,  3447,\n",
       "           1996,  2087,  5851,  3675,  1999,  2381,  1012,  2035,  2027,\n",
       "           2018,  2000,  2079,  2001,  2562,  2023,  5744,  1011,  2770,\n",
       "           2291,  2006,  8285,  8197, 10994,  1012,  2612,  1010,  1999,\n",
       "           1996,  8487,  1997,  1037,  2074,  2261,  3134,  1010,  1996,\n",
       "           7226,  2368,  3447,  2038,   102], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1], dtype=int32)},\n",
       "  1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfdataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61644f25-8158-481d-9aa7-4e370dc4dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "355b1123-3516-4c12-b8c5-e6bc211a4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(x) * (1-TEST_SPLIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a81cb4d-4105-4667-8383-919f4366249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8570043-1c05-4199-8f17-b8e9ed7465e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ecc56865-7703-4889-82fe-9d1c865204ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### firstly shuffle\n",
    "#### then take the train and the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42572823-36b4-4e6b-8e15-de4f134f6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdataset_shuffled = tfdataset.shuffle(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "487cbef5-7b65-4eda-92ec-17a3f946b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdataset_train = tfdataset_shuffled.take(train_size)\n",
    "tfdataset_test = tfdataset_shuffled.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b4c9f6a-4322-4a5c-99e7-fc56b14be177",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32325d0f-af19-49d9-8a58-b24ed87c9b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc54984b-79b6-49ac-b748-79915a352b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "37/37 [==============================] - 30s 563ms/step - loss: 0.6359 - accuracy: 0.6486\n",
      "Epoch 2/2\n",
      "37/37 [==============================] - 21s 574ms/step - loss: 0.5356 - accuracy: 0.7432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb02ef7fa60>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# set up model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# define optimizer to be used to minimise loss\n",
    "optimizer = optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "# define loss function\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = loss,\n",
    "              metrics = \"accuracy\")\n",
    "# fit model\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "893d7fa0-d7a5-4841-bba1-43d7a1d257b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 83ms/step - loss: 0.4971 - accuracy: 0.6842\n",
      "{'loss': 0.49705132842063904, 'accuracy': 0.6842105388641357}\n"
     ]
    }
   ],
   "source": [
    "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62162e56-8b2f-498d-ad57-65bdbbe7c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the accuracy is not very good, as it is only 0.54. Baseline accuracy = 0.5\n",
    "# could probably improve the accuracy by increasing the length of each article? (currently set to 50 words)\n",
    "# accuracy will also be increased with better text preprocessing?\n",
    "# accuracy could also be increased with more epochs (but this takes a long time) - could train on google Colab instead?\n",
    "# should also set it up so that it uses validation splits in the model compilation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd6be72c-e5b0-4ce7-92ac-30e83091e6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 6s 56ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.44988844, -0.5441926 ],\n",
       "       [ 0.95785654, -0.8580198 ],\n",
       "       [ 0.91471153, -0.9145882 ],\n",
       "       [ 0.9522299 , -0.75392073],\n",
       "       [ 0.86895686, -0.7939154 ],\n",
       "       [ 0.9253699 , -0.7915701 ],\n",
       "       [ 0.13305412, -0.17068581],\n",
       "       [ 0.78369665, -0.6722093 ],\n",
       "       [ 0.94184816, -0.8654311 ],\n",
       "       [ 0.94227445, -0.79016227],\n",
       "       [ 0.9896671 , -0.8564901 ],\n",
       "       [ 0.83368087, -0.7301311 ],\n",
       "       [ 0.28876665, -0.19126669],\n",
       "       [ 0.84619594, -0.7681285 ],\n",
       "       [ 0.82398105, -0.72452235],\n",
       "       [ 0.4245253 , -0.45559487],\n",
       "       [ 0.3714038 , -0.33988434],\n",
       "       [ 0.9767426 , -0.8168302 ],\n",
       "       [ 0.909147  , -0.8066562 ],\n",
       "       [ 0.826403  , -0.7789222 ],\n",
       "       [ 0.22265616, -0.2508639 ],\n",
       "       [ 0.86053973, -0.755985  ],\n",
       "       [ 0.83074564, -0.80895734],\n",
       "       [ 0.9783216 , -0.84626067],\n",
       "       [ 0.8926482 , -0.81569695],\n",
       "       [ 0.7530662 , -0.6505507 ],\n",
       "       [ 0.6009081 , -0.61131436],\n",
       "       [ 0.853378  , -0.7526596 ],\n",
       "       [ 0.25482547, -0.35379758],\n",
       "       [ 0.9296929 , -0.82111377],\n",
       "       [ 0.8898216 , -0.7842747 ],\n",
       "       [ 0.37472856, -0.27817902],\n",
       "       [ 0.80560404, -0.7199923 ],\n",
       "       [ 0.950927  , -0.8180686 ],\n",
       "       [ 0.9215559 , -0.8166644 ],\n",
       "       [ 0.979238  , -0.8324595 ],\n",
       "       [ 0.41414848, -0.398697  ],\n",
       "       [ 0.39659852, -0.460336  ],\n",
       "       [ 0.8916839 , -0.82334447],\n",
       "       [ 0.90071064, -0.78986835],\n",
       "       [ 0.03892539, -0.06035753],\n",
       "       [ 0.9290998 , -0.7394117 ],\n",
       "       [ 0.7068737 , -0.7066484 ],\n",
       "       [ 0.9629313 , -0.75997156],\n",
       "       [ 0.9848997 , -0.8176197 ],\n",
       "       [ 0.9771003 , -0.8786406 ],\n",
       "       [ 0.9675957 , -0.7813441 ],\n",
       "       [ 0.8985917 , -0.80935204],\n",
       "       [ 0.76891756, -0.78649855],\n",
       "       [ 0.37680802, -0.34437636],\n",
       "       [ 0.3462146 , -0.31038612],\n",
       "       [ 0.8079601 , -0.74121445],\n",
       "       [ 0.92668325, -0.87005025],\n",
       "       [ 0.9357422 , -0.8144223 ],\n",
       "       [ 0.32478505, -0.24677883],\n",
       "       [ 0.8773984 , -0.86345345],\n",
       "       [ 0.800994  , -0.7569458 ],\n",
       "       [ 0.68221104, -0.6816577 ],\n",
       "       [ 0.30544078, -0.2514278 ],\n",
       "       [ 0.8800949 , -0.78961664],\n",
       "       [ 0.97065264, -0.8208352 ],\n",
       "       [ 0.82086945, -0.73360175],\n",
       "       [ 0.8547748 , -0.7748562 ],\n",
       "       [ 0.3577703 , -0.22398642],\n",
       "       [ 0.8809499 , -0.7830971 ],\n",
       "       [ 0.66834074, -0.66507167],\n",
       "       [ 0.4898617 , -0.37932238],\n",
       "       [ 0.7603019 , -0.6740859 ],\n",
       "       [ 0.3563841 , -0.31403092],\n",
       "       [ 0.56080896, -0.4972644 ],\n",
       "       [ 0.97075576, -0.8652042 ],\n",
       "       [ 0.90097564, -0.7979513 ],\n",
       "       [ 0.83904916, -0.7503367 ],\n",
       "       [ 0.10842729, -0.18529248],\n",
       "       [ 0.5824696 , -0.5424995 ],\n",
       "       [ 0.9582021 , -0.8676961 ],\n",
       "       [ 1.0164121 , -0.82179403],\n",
       "       [ 0.96081716, -0.81151193],\n",
       "       [ 0.84491867, -0.7834035 ],\n",
       "       [ 0.9231865 , -0.79641443],\n",
       "       [ 0.95209473, -0.8343375 ],\n",
       "       [ 1.0053911 , -0.8509149 ],\n",
       "       [ 0.97274697, -0.88169414],\n",
       "       [ 0.87992626, -0.800678  ],\n",
       "       [ 0.9241259 , -0.81798875],\n",
       "       [ 0.03892539, -0.06035753],\n",
       "       [ 0.74489045, -0.7086816 ],\n",
       "       [ 0.7329488 , -0.85261863],\n",
       "       [ 0.8287682 , -0.7345345 ],\n",
       "       [ 0.9712901 , -0.84449   ],\n",
       "       [ 0.72931886, -0.72715664],\n",
       "       [ 0.21204531, -0.20870492],\n",
       "       [ 0.32613638, -0.25176436]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdataset = tf.data.Dataset.from_tensor_slices(dict(tokens))\n",
    "testing = model.predict(tfdataset)\n",
    "testing[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b5e0153-4478-47fe-a9fc-d57666dfb0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0534004 ,  0.02739107], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = testing[0][0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8bd02-7e5c-436f-83d0-cfbf505f7c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "701a4a72-0f18-470b-9a71-c025b4056898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.4798131, 0.5201869], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c747533-3e2e-42e0-b9b2-1fb0860d048b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb485d09-7605-4792-a276-19678bfa7244",
   "metadata": {},
   "source": [
    "## TESTING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88c1ae32-93e0-4c8d-8ed4-233c99379ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 3e-5\n",
    "TOKEN_MAX_LEN = 50\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23c8a0b7-5caa-4a74-8ddd-8460cf316ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db725f-304d-42eb-ab10-7bff0fd5629d",
   "metadata": {},
   "source": [
    "### testing the text_tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9667a071-9db8-4381-b0bd-0f7d669fea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer(X,\n",
    "                   tokenizer,\n",
    "                   max_len = TOKEN_MAX_LEN,\n",
    "                   truncation = True,\n",
    "                   padding = \"max_length\"):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of tokenized text with 2 keys: \"input_ids\" and \"attention_mask\".\n",
    "    These 2 keys are required for the input to the DistilBert model.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer(X, max_length = max_len, truncation = truncation, padding = padding)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "565d29ad-3103-4bf9-8e84-15637636cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text_tokenizer(x, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e23778e1-0269-45c9-b77f-4b5ba3edf79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1996, 2142, 3741, 2303, 3625, 2005, 19642, 9302, 13141, 2495, 2056, 3728, 2009, 2018, 3718, 6355, 1998, 3424, 1011, 3956, 4180, 2013, 2569, 4475, 2009, 2405, 2000, 2393, 9302, 2336, 2817, 2012, 2188, 2076, 1996, 2522, 17258, 1011, 2539, 6090, 3207, 7712, 1012, 2023, 8874, 2234, 2044, 2009, 102], [101, 4735, 12087, 1037, 3394, 4058, 3232, 2040, 2587, 1996, 11292, 24018, 2031, 2042, 5338, 2007, 9714, 2005, 9382, 20699, 2005, 2706, 2007, 2060, 2372, 1997, 1996, 2521, 1011, 2157, 8396, 2177, 2000, 4040, 1996, 1057, 1012, 1055, 1012, 9424, 1012, 12834, 6262, 1010, 3438, 1010, 1998, 2014, 3129, 102], [101, 1523, 2057, 1521, 2128, 2085, 2893, 2062, 1997, 2122, 28616, 1011, 1039, 4268, 1010, 2021, 2023, 2051, 1010, 2009, 2074, 3849, 2008, 1037, 3020, 7017, 1997, 2068, 2024, 2428, 11321, 5665, 1010, 1524, 2056, 2852, 1012, 23455, 2139, 11607, 5332, 1010, 2708, 1997, 16514, 7870, 2012, 2336, 1521, 102], [101, 4001, 8037, 2024, 6183, 2000, 5290, 1996, 10882, 29521, 19966, 2121, 1999, 3433, 2000, 2086, 1997, 14254, 8370, 7878, 1517, 2021, 10643, 2123, 1005, 1056, 4025, 15241, 4986, 2055, 1996, 9824, 2044, 9358, 15061, 12411, 1012, 3533, 2158, 17231, 1010, 1040, 1011, 1059, 1012, 12436, 1012, 1010, 5451, 102], [101, 2028, 3114, 8398, 6793, 2903, 2010, 3658, 3310, 2013, 1037, 3937, 2755, 2055, 1996, 4167, 1024, 2009, 3138, 2062, 5177, 3947, 2000, 15454, 2019, 2801, 2004, 6270, 2084, 2000, 5138, 2009, 2004, 2995, 1012, 1999, 2060, 2616, 1010, 2009, 1521, 1055, 6082, 2000, 2903, 2084, 2000, 2025, 1012, 102], [101, 1045, 2572, 2559, 2012, 1037, 2158, 2040, 2003, 2200, 15560, 1012, 2002, 2003, 1037, 8003, 1998, 1037, 2810, 7309, 1998, 2515, 2025, 2215, 2000, 2175, 2000, 1996, 2902, 1012, 2002, 19981, 2015, 2012, 2033, 1998, 2026, 2147, 4256, 2004, 2057, 3233, 2011, 2256, 7683, 2121, 1999, 1996, 20629, 102], [101, 2343, 3235, 3158, 29470, 1010, 1039, 1012, 7313, 1011, 6889, 1006, 3075, 1997, 3519, 1007, 8130, 2039, 2007, 3235, 3158, 29470, 1999, 4027, 1521, 1055, 5239, 1010, 6108, 1998, 5355, 7637, 1996, 2210, 16669, 2015, 4125, 2000, 1996, 3580, 8798, 1010, 2010, 2576, 5442, 3554, 2007, 2198, 1039, 102], [101, 7145, 1010, 1996, 5404, 3293, 2038, 2146, 2042, 1037, 3287, 1011, 6817, 5884, 1012, 2043, 2308, 2020, 2443, 1010, 2009, 2001, 3952, 2004, 3239, 9485, 2030, 1037, 8257, 5080, 1517, 2030, 2119, 1012, 2085, 1010, 1037, 2047, 2002, 3170, 7520, 4748, 2003, 13659, 2000, 4119, 1996, 22807, 1997, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 20996, 18684, 2638, 5637, 2003, 2028, 1997, 1996, 2087, 12807, 1998, 22979, 4898, 1997, 2256, 4245, 1012, 2014, 2573, 102], [101, 4942, 29234, 2000, 1996, 3842, 4942, 29234, 2085, 2005, 2004, 2210, 2004, 1002, 1016, 1037, 3204, 999, 4942, 29234, 2085, 2005, 2004, 2210, 2004, 1002, 1016, 1037, 3204, 999, 2131, 1996, 3842, 1521, 1055, 4882, 17178, 26587, 1012, 1037, 4882, 17886, 1997, 1996, 2190, 1997, 2256, 6325, 1012, 102], [101, 2216, 2040, 2031, 2499, 1999, 1037, 2586, 5814, 3269, 3325, 2178, 2088, 2043, 2027, 2024, 4846, 2012, 1037, 2512, 19496, 2239, 4322, 1012, 2009, 2001, 2037, 2586, 3325, 2008, 12774, 2093, 2280, 2586, 2372, 2000, 10939, 14017, 3567, 2100, 1521, 1055, 2512, 19496, 2239, 18880, 1010, 3146, 1010, 102], [101, 1037, 1523, 3892, 2265, 1524, 6903, 2165, 1037, 2735, 2023, 2733, 2043, 4113, 2198, 6291, 7463, 1037, 3232, 1997, 25007, 3980, 2012, 9733, 1521, 1055, 24969, 1012, 6291, 1998, 3677, 5261, 16443, 3879, 1999, 1996, 10694, 1523, 4931, 8957, 1524, 6903, 2000, 2131, 1996, 7484, 2376, 3353, 2000, 102], [101, 1996, 6236, 2533, 2056, 9317, 2009, 2038, 2366, 4942, 6873, 26474, 2006, 3674, 2822, 3316, 2008, 3073, 2592, 1998, 4806, 2974, 2578, 1999, 1996, 2142, 2163, 2000, 2156, 2065, 2027, 13382, 1037, 2120, 3036, 3891, 1012, 1000, 7211, 2038, 5117, 1999, 6204, 2008, 14969, 2015, 2256, 10660, 3341, 102], [101, 2795, 1997, 8417, 2045, 2003, 1037, 2146, 2381, 1997, 2231, 8830, 1999, 2943, 6089, 1012, 3365, 2943, 21762, 4839, 1999, 1996, 1057, 1012, 1055, 1012, 4171, 3642, 2000, 5326, 2030, 4942, 5332, 4305, 4371, 1996, 2537, 1997, 10036, 1998, 12990, 10725, 2943, 1012, 2070, 1997, 2122, 21762, 2031, 102], [101, 18527, 1010, 2030, 3383, 11414, 2135, 1010, 1996, 1055, 1012, 1040, 1012, 1055, 1012, 4680, 2012, 2029, 13872, 1521, 1055, 4861, 2001, 4233, 2001, 2218, 2012, 2019, 4547, 3409, 1999, 3417, 21899, 1010, 4174, 1010, 2008, 2018, 2042, 13190, 2000, 1996, 2177, 2011, 1996, 2142, 8285, 3667, 1012, 102], [101, 4522, 4200, 2003, 1037, 2828, 1997, 3949, 2008, 4247, 2000, 4982, 1999, 6217, 1012, 2411, 2009, 2950, 13441, 2008, 4995, 1005, 1056, 2047, 1998, 2070, 2111, 2453, 2130, 2655, 2068, 3418, 1012, 2144, 2049, 12149, 1010, 2116, 2031, 2170, 2715, 4200, 1037, 5592, 2013, 2643, 1012, 2823, 1010, 102], [101, 1996, 14420, 2038, 16981, 2048, 3146, 10093, 14545, 25074, 7747, 1002, 14993, 2454, 2005, 2437, 5560, 1015, 4551, 6487, 24755, 12718, 1010, 17067, 2111, 2408, 1996, 2142, 2163, 1012, 1996, 3316, 1517, 2029, 2031, 2363, 1996, 2922, 2986, 1999, 14420, 2381, 1517, 1523, 17800, 11867, 21511, 2098, 1010, 102], [101, 3507, 9733, 7066, 1024, 1045, 1521, 1049, 7568, 2000, 14970, 2008, 2023, 1053, 2509, 1045, 1521, 2222, 6653, 2000, 3237, 3242, 1997, 1996, 9733, 2604, 1998, 5557, 14855, 4757, 2100, 2097, 2468, 5766, 1012, 1999, 1996, 4654, 8586, 3242, 2535, 1010, 1045, 13566, 2000, 3579, 2026, 19320, 1998, 102], [101, 1037, 2047, 3189, 2013, 14955, 18291, 2080, 2023, 2733, 2699, 2000, 8328, 2422, 2006, 1037, 9575, 2116, 14009, 2031, 4384, 1999, 1996, 2317, 2160, 1012, 3580, 2343, 21911, 2050, 5671, 3849, 2000, 2031, 1037, 2062, 2430, 1998, 13155, 2535, 1999, 1996, 2047, 3447, 2084, 2003, 4050, 22891, 2000, 102], [101, 1037, 2688, 2038, 2042, 3140, 2000, 6366, 2049, 6231, 1997, 2280, 2343, 6221, 8398, 2004, 1037, 2765, 1997, 2009, 2108, 11696, 2011, 5731, 1012, 2429, 2000, 3331, 2685, 24443, 1010, 3434, 10722, 11488, 6784, 1005, 1055, 13844, 9316, 1999, 2624, 4980, 1010, 3146, 1010, 2038, 3718, 2049, 8398, 102], [101, 4079, 1004, 24253, 2003, 4640, 1037, 2047, 2336, 1521, 1055, 2338, 2008, 2097, 8439, 1996, 2166, 1997, 2852, 1012, 4938, 6904, 14194, 2072, 2011, 11888, 2989, 2010, 6613, 24615, 1998, 16907, 2010, 2966, 2476, 2000, 2468, 1523, 2637, 1521, 1055, 3460, 1012, 1524, 2852, 1012, 6904, 14194, 2072, 102], [101, 2004, 1996, 3795, 3947, 2000, 3745, 28896, 8039, 2152, 6718, 1010, 2009, 2097, 2145, 2022, 2086, 2077, 25501, 1997, 2111, 2064, 2131, 1996, 2915, 1517, 8701, 2062, 6677, 1998, 1037, 2936, 4990, 2000, 3671, 5666, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 5344, 1999, 2023, 3746, 2024, 1996, 5344, 1997, 2111, 2057, 1521, 2310, 2439, 2000, 2522, 17258, 1011, 2539, 1012, 2089, 2027, 2717, 1999, 3521, 1012, 2130, 2004, 2019, 7861, 15069, 1010, 2009, 1521, 1055, 2524, 2000, 5674, 1996, 2156, 20744, 19006, 1010, 13346, 2791, 1010, 12721, 1010, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2006, 15060, 1045, 2228, 1997, 2026, 7133, 1517, 1037, 5189, 1010, 2785, 1010, 16405, 16989, 18436, 2450, 2040, 28432, 102], [101, 2076, 1996, 8398, 2086, 1010, 1996, 7655, 1000, 6502, 2733, 1000, 8369, 2041, 2004, 1037, 4066, 1997, 2598, 25852, 2154, 1011, 2806, 8595, 4179, 1012, 2054, 2211, 1999, 2238, 2418, 2004, 1037, 3478, 3947, 2011, 1996, 6221, 1005, 1055, 2317, 2160, 1998, 1037, 3951, 4001, 2000, 3579, 2006, 102], [101, 2047, 3290, 3951, 29490, 10337, 2003, 1058, 14147, 2005, 1996, 3728, 15348, 1057, 1012, 1055, 1012, 2160, 2835, 1999, 2014, 2110, 1010, 4129, 3677, 5487, 16694, 2006, 23466, 2595, 2213, 1521, 1055, 7987, 20175, 8237, 2102, 2739, 5095, 2016, 18754, 2000, 2954, 2005, 1996, 2551, 2465, 1998, 4337, 102], [101, 4841, 2024, 2025, 3403, 2005, 6656, 2000, 3604, 1012, 1996, 6401, 2005, 4295, 2491, 1998, 9740, 2145, 18012, 2114, 3604, 2021, 2951, 2013, 2233, 3065, 4841, 2024, 3500, 2075, 3805, 2007, 3604, 3488, 1012, 3309, 21725, 2015, 3123, 2000, 2037, 3284, 2504, 1999, 2058, 1037, 2095, 1010, 2951, 102], [101, 27754, 2906, 2139, 2744, 2891, 24664, 3406, 8529, 9026, 2019, 2080, 1010, 1051, 19115, 2566, 3207, 2226, 14163, 9956, 2015, 9706, 25463, 7983, 2229, 2053, 2345, 2139, 12609, 1012, 18499, 26354, 2080, 1010, 3653, 22987, 22591, 2015, 19817, 10936, 2121, 14736, 2015, 1015, 1012, 2199, 21877, 24137, 3022, 102], [101, 4199, 1517, 2058, 3263, 2446, 1011, 4092, 3234, 29013, 3843, 1037, 4861, 4465, 21936, 1996, 12111, 1521, 1055, 3522, 8170, 26325, 1996, 13301, 1997, 2168, 1011, 3348, 9209, 1012, 1996, 12111, 3793, 1523, 2003, 7356, 2011, 1037, 15112, 6553, 9218, 1997, 19113, 1998, 5860, 20026, 28184, 2114, 15667, 102], [101, 2129, 2097, 7862, 3342, 6221, 8398, 1029, 2028, 3634, 2086, 3283, 1010, 2637, 3053, 5707, 2046, 1037, 18944, 1012, 1996, 3842, 2001, 5552, 3952, 2011, 2048, 2477, 1024, 1996, 2755, 2008, 1996, 2158, 2040, 6257, 2000, 2022, 2049, 14870, 21237, 1010, 2343, 6221, 8398, 1010, 2001, 2205, 5236, 102], [101, 6221, 8398, 1998, 2010, 9585, 2869, 2031, 2042, 2437, 4447, 1997, 6923, 14303, 9861, 1010, 23294, 8817, 1997, 2111, 2024, 6830, 17800, 1999, 2344, 2000, 19838, 2256, 3864, 1012, 28352, 17791, 1012, 2292, 1521, 1055, 2298, 2012, 1996, 8866, 1998, 2139, 8569, 8950, 2037, 17218, 2320, 1998, 2005, 102], [101, 2343, 3533, 7226, 2368, 1521, 1055, 2933, 2000, 2128, 15222, 8950, 1996, 2142, 2163, 1521, 3921, 2000, 2859, 2071, 2067, 10273, 1010, 2360, 8519, 1010, 2040, 11582, 2008, 2859, 2097, 2298, 2005, 3971, 2000, 2202, 5056, 1997, 1996, 1057, 1012, 1055, 1012, 2096, 7226, 2368, 11333, 18142, 2015, 102], [101, 1996, 2679, 2003, 2006, 999, 2296, 2602, 3049, 5402, 2057, 2360, 2008, 2023, 2003, 1996, 2087, 2590, 2602, 1997, 2256, 3268, 1011, 1011, 1011, 2030, 1999, 2137, 2381, 1012, 2339, 1029, 1517, 2138, 2009, 1521, 1055, 2995, 1012, 2018, 22744, 7207, 2180, 1999, 2355, 1010, 2049, 2825, 2340, 102], [101, 2106, 5205, 6945, 8096, 4019, 1996, 3146, 7071, 2007, 2010, 2564, 1998, 2048, 2402, 2336, 2011, 3909, 2000, 2064, 10841, 2078, 1998, 2681, 2369, 1996, 2155, 3899, 1029, 8096, 2513, 2000, 3146, 2023, 3944, 1010, 2044, 1037, 6398, 2074, 2044, 7090, 9432, 2851, 3936, 1996, 3146, 5205, 2001, 102], [101, 2053, 1010, 2023, 2025, 1037, 2919, 3959, 1012, 2017, 2428, 2106, 2074, 5256, 2039, 1999, 2122, 1000, 2307, 1000, 2142, 2163, 2000, 2424, 2008, 2053, 3043, 2073, 2017, 2444, 1010, 2115, 4071, 1997, 2929, 2003, 2085, 9530, 3367, 20623, 1012, 2025, 2011, 1996, 2510, 1010, 2021, 2011, 1996, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2004, 2057, 4607, 1996, 2345, 7683, 1997, 16080, 2077, 1996, 2602, 1010, 1037, 8068, 2685, 2033, 2000, 1037, 9762, 102], [101, 10439, 9722, 12849, 9153, 3449, 29218, 5886, 3695, 2226, 1010, 2040, 3130, 2170, 3086, 2000, 1996, 3291, 1997, 8040, 3286, 18726, 2006, 6207, 1521, 1055, 16380, 10439, 3573, 1010, 2038, 6406, 1037, 9870, 2114, 6207, 1999, 2662, 16723, 1996, 2194, 1997, 18077, 2075, 2049, 15404, 2373, 2058, 18726, 102], [101, 18346, 1010, 2605, 1006, 9706, 1007, 1516, 25209, 2050, 1521, 1055, 2413, 7506, 1998, 2195, 1997, 2049, 12706, 2024, 2275, 2000, 2175, 2006, 3979, 6928, 2058, 13519, 2008, 2027, 17800, 11867, 6340, 2006, 5126, 1998, 6304, 1012, 3119, 9209, 2988, 1996, 7390, 1998, 2188, 5350, 2194, 2000, 2413, 102], [101, 2343, 6221, 8398, 14429, 2135, 2170, 2110, 4905, 2236, 8226, 7148, 18940, 17668, 4590, 1010, 2022, 15172, 2032, 2005, 2010, 3279, 1997, 4108, 1998, 9694, 2002, 1000, 2424, 1000, 2032, 2340, 1010, 2199, 2062, 4494, 1999, 2054, 2070, 3423, 8519, 2360, 2001, 1037, 4735, 2552, 1012, 2021, 2625, 102], [101, 1999, 2048, 3025, 8466, 1010, 3021, 9587, 10532, 2015, 1010, 4988, 2585, 8512, 2100, 1010, 1998, 2500, 2956, 1999, 1996, 2047, 4516, 1010, 22889, 4710, 1996, 5202, 1010, 4541, 2129, 14926, 2386, 4063, 2075, 2038, 6964, 2499, 1998, 2054, 2904, 1999, 2230, 1012, 3951, 22680, 2109, 2417, 2923, 102], [101, 1996, 2739, 17848, 4803, 24304, 2015, 2862, 2097, 2079, 2062, 2084, 23216, 2115, 2568, 1012, 2122, 9631, 2089, 4119, 2115, 9029, 1010, 5041, 2368, 2115, 15251, 1010, 4654, 17847, 2115, 12731, 9488, 24279, 1010, 2030, 21255, 2115, 9647, 1012, 2122, 2808, 2089, 2025, 9352, 3711, 2006, 1996, 2880, 102], [101, 18856, 24237, 2015, 1997, 4907, 9247, 8496, 2031, 10538, 2039, 1999, 2115, 4220, 1010, 2061, 2017, 3362, 2005, 1037, 5835, 1997, 2461, 6279, 1010, 1996, 2759, 17901, 6359, 1012, 2009, 2003, 2124, 2005, 2108, 2200, 4621, 1010, 2021, 2049, 2364, 21774, 1010, 1043, 2135, 8458, 8820, 2618, 1010, 102], [101, 2280, 2413, 2343, 9473, 18906, 3683, 9096, 8480, 2012, 1996, 20747, 6928, 1010, 2233, 1015, 1010, 25682, 1999, 3000, 1012, 1996, 14392, 2003, 3517, 1999, 1037, 8637, 7897, 1998, 3747, 1011, 21877, 21814, 3979, 2008, 2038, 2404, 2413, 2280, 2343, 9473, 18906, 3683, 9096, 2012, 3891, 1997, 1037, 102], [101, 1996, 2343, 1521, 1055, 4740, 2000, 18793, 1999, 2019, 7552, 4812, 2071, 3815, 2000, 27208, 1997, 3425, 2030, 2060, 4735, 13302, 1010, 3423, 8519, 2056, 1010, 2295, 2027, 14046, 2098, 1037, 2553, 2071, 2022, 3697, 2000, 6011, 1012, 4748, 4748, 3187, 1997, 2110, 8226, 7148, 18940, 17668, 4590, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2023, 2466, 2001, 2761, 2405, 2011, 6151, 17007, 1998, 2003, 22296, 2182, 2004, 2112, 1997, 1996, 4785, 4624, 5792, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 1996, 2254, 1020, 2886, 2006, 1996, 9424, 2001, 1010, 2005, 2116, 4841, 1010, 2019, 4895, 15222, 25804, 3468, 1998, 102], [101, 2023, 2466, 2001, 2761, 2405, 2011, 24665, 2923, 1998, 2003, 22296, 2182, 2004, 2112, 1997, 1996, 4785, 4624, 5792, 1012, 2041, 1997, 1996, 3420, 1997, 2026, 3239, 1010, 1045, 3236, 1037, 12185, 1997, 4714, 1017, 1011, 2095, 1011, 2214, 8038, 4313, 19021, 19585, 2080, 24770, 2067, 1998, 5743, 102], [101, 2616, 18953, 2011, 1996, 6075, 2008, 2343, 8398, 2038, 2109, 2068, 1999, 1056, 28394, 3215, 2076, 2010, 2744, 1999, 2436, 1010, 2429, 2000, 1996, 8398, 10474, 8756, 1012, 2899, 1517, 2343, 6221, 8398, 2985, 2172, 1997, 1996, 12609, 4883, 3049, 22604, 2008, 2002, 2071, 2069, 4558, 2065, 1996, 102], [101, 16836, 2013, 2430, 2637, 1998, 2037, 2336, 3422, 2019, 6912, 4146, 2011, 2372, 1997, 1996, 2149, 3675, 6477, 2012, 1996, 2624, 1061, 5332, 22196, 5153, 1999, 2624, 5277, 1012, 2149, 3675, 4584, 2218, 2062, 2084, 1019, 1010, 2199, 14477, 21408, 8737, 7088, 2098, 11560, 2336, 1999, 2037, 9968, 102], [101, 4831, 4557, 2741, 1037, 23921, 2000, 1996, 3059, 2343, 9857, 14026, 2010, 14038, 2058, 1996, 10102, 6928, 1997, 1996, 2406, 1521, 1055, 6059, 2000, 1996, 3537, 3072, 1997, 1996, 9030, 1012, 1523, 2009, 2001, 2007, 14038, 2008, 1045, 4342, 1997, 1996, 13800, 2886, 1999, 1996, 3537, 3072, 1997, 102], [101, 2004, 1996, 1044, 18863, 2105, 2512, 1011, 15289, 3468, 19204, 1006, 1050, 6199, 1007, 2396, 4247, 2000, 4982, 1010, 2116, 2024, 9436, 2594, 18639, 2290, 1996, 13675, 10936, 2063, 2164, 2329, 3364, 1998, 9971, 2198, 18856, 10285, 2063, 2040, 2003, 2085, 4855, 2010, 2219, 5059, 1997, 1996, 6613, 102], [101, 2023, 2733, 3677, 19379, 14634, 26234, 7566, 2000, 5660, 8654, 3166, 6423, 10722, 2869, 10222, 2055, 2014, 2476, 2004, 1037, 2658, 2188, 5660, 1998, 2014, 2047, 2338, 3432, 6423, 1024, 7287, 3733, 19328, 2005, 7965, 7216, 2833, 1012, 1999, 1996, 4357, 1010, 6423, 7607, 2339, 2016, 2001, 4567, 102], [101, 2043, 1996, 24265, 2015, 2234, 2091, 2114, 2280, 8398, 3049, 3472, 2703, 24951, 13028, 2703, 2198, 24951, 13028, 21572, 3366, 12690, 5668, 4530, 3947, 2000, 15126, 2093, 24951, 13028, 5144, 2044, 8398, 14933, 8495, 4107, 1014, 2243, 10377, 2005, 2845, 3275, 11382, 17960, 8238, 2047, 2259, 2457, 3513, 102], [101, 16360, 1012, 3958, 5207, 2508, 1006, 3958, 1007, 3817, 5207, 2890, 14289, 16558, 5555, 3619, 2655, 2005, 4994, 2006, 3675, 12058, 1005, 17542, 3226, 1005, 2003, 2074, 2489, 4613, 3173, 2500, 26771, 11721, 26327, 1010, 5207, 3198, 2160, 14814, 2000, 15113, 9530, 8043, 22879, 5668, 19801, 1010, 8951, 102], [101, 1006, 6646, 2581, 2581, 2620, 1013, 2131, 3723, 4871, 1007, 2006, 2010, 3167, 9927, 1010, 2934, 2726, 3044, 1997, 1996, 2118, 1997, 2624, 5277, 2375, 2082, 2626, 1037, 2695, 2008, 2001, 9249, 4187, 1997, 2822, 2231, 6043, 1012, 3859, 6920, 1010, 1996, 3834, 11240, 5496, 2032, 1997, 5636, 102], [101, 2057, 2064, 2644, 3403, 2005, 1996, 2502, 6543, 5325, 1010, 1996, 4937, 6305, 2135, 6491, 2594, 6092, 12554, 1010, 1996, 25312, 19159, 1997, 7072, 2008, 2116, 2031, 2146, 8615, 1999, 1996, 2142, 2163, 1997, 2637, 1012, 1996, 5325, 2003, 2525, 2182, 1010, 1998, 2057, 1005, 2310, 2042, 2542, 102], [101, 1045, 1521, 1040, 2196, 2657, 1997, 1996, 20907, 2265, 18353, 4890, 1004, 4108, 2127, 3041, 2023, 3204, 1010, 2043, 4202, 9170, 7549, 2009, 2005, 1037, 1523, 13971, 1010, 6171, 3348, 2923, 1524, 8257, 1999, 1037, 1056, 28394, 2102, 2000, 2014, 6070, 2454, 1011, 4606, 8771, 1012, 2076, 1996, 102], [101, 1037, 12720, 2610, 5290, 3021, 2979, 2011, 2160, 8037, 2247, 2283, 3210, 2071, 2404, 2610, 3738, 1999, 4795, 8146, 1010, 1998, 2031, 1037, 4997, 4254, 2006, 14357, 4073, 1010, 2975, 4279, 2008, 2024, 21834, 2094, 2007, 4126, 2130, 2062, 8211, 1012, 3145, 3787, 1997, 1996, 1000, 2577, 12305, 102], [101, 1037, 28475, 2007, 1996, 2004, 6494, 10431, 19281, 1521, 1055, 21887, 23350, 17404, 1999, 4068, 1010, 2762, 1010, 2233, 2385, 1010, 25682, 1012, 1006, 24181, 7003, 2818, 3489, 1013, 26665, 1007, 2651, 2006, 1996, 10195, 1010, 4138, 1010, 4918, 1010, 20789, 1010, 1998, 3958, 6848, 1996, 6745, 2039, 102], [101, 1996, 2034, 2733, 1997, 2254, 2001, 4417, 2011, 2048, 3278, 2824, 1999, 2149, 4331, 1024, 1996, 2448, 1011, 2125, 2602, 9248, 1999, 4108, 1997, 8037, 12551, 11582, 7432, 1998, 2198, 9808, 6499, 4246, 1010, 3228, 1996, 2283, 2491, 1997, 1996, 2149, 4001, 1025, 1998, 1996, 5274, 1997, 1996, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2045, 2024, 2116, 10487, 2005, 3183, 3153, 2003, 2037, 9518, 1998, 2379, 1011, 13048, 6693, 1010, 2040, 3345, 2005, 102], [101, 1996, 2047, 3795, 3189, 4455, 1010, 2426, 2500, 1010, 2005, 5509, 4073, 2000, 4769, 4803, 2300, 6911, 1998, 5335, 1996, 8122, 1997, 2300, 2224, 1999, 5237, 1998, 3068, 1010, 1998, 2009, 22106, 4506, 1999, 2093, 2752, 1024, 2034, 1010, 12067, 2111, 2000, 15581, 2000, 1996, 14670, 1997, 4785, 102], [101, 8003, 3743, 4988, 9152, 22842, 6216, 6769, 2038, 2081, 2381, 2012, 6568, 2011, 3352, 1996, 2034, 2304, 2450, 2000, 3677, 1523, 4024, 3892, 1012, 1524, 6769, 4354, 2014, 2034, 2792, 2006, 9432, 4077, 4901, 26551, 1010, 2040, 2003, 2036, 2304, 1012, 2016, 2003, 1996, 2034, 2450, 1997, 3609, 102], [101, 2047, 2259, 3099, 4080, 12731, 19506, 8847, 2076, 1037, 3679, 27918, 2206, 1996, 8293, 1997, 1996, 21887, 23350, 1999, 2047, 2259, 2103, 1010, 2251, 2410, 1010, 12609, 1012, 1006, 3505, 16562, 2099, 1013, 26665, 1007, 2651, 2006, 1996, 12584, 3189, 1010, 5557, 1998, 4138, 6848, 1996, 2047, 2259, 102], [101, 1996, 18297, 2187, 1996, 4112, 1998, 1037, 8343, 5303, 2044, 7493, 3674, 7171, 1012, 1011, 1011, 1037, 4108, 6458, 1005, 1055, 4112, 2003, 18836, 2005, 2010, 2166, 2044, 25620, 2543, 2007, 1037, 8343, 2096, 2006, 1037, 7574, 2655, 2012, 1037, 2797, 5039, 1999, 3541, 19817, 7140, 2361, 2221, 102], [101, 1999, 4507, 1010, 4004, 2015, 2024, 6524, 2641, 2317, 1010, 1998, 1996, 2944, 1011, 7162, 10661, 14485, 2015, 1996, 6565, 5966, 2426, 4004, 1011, 4841, 1012, 2054, 1521, 1055, 2062, 1010, 1996, 10661, 3271, 2000, 12919, 2637, 1521, 1055, 2317, 4314, 2344, 1010, 2029, 4447, 2000, 27329, 8906, 102], [101, 1037, 2177, 1997, 16836, 2770, 2046, 1996, 4534, 1997, 2474, 23417, 7153, 2019, 4357, 2090, 7987, 20175, 8237, 2102, 3146, 1998, 1057, 1012, 1055, 1012, 12295, 9090, 6060, 1006, 1054, 1011, 19067, 1007, 2040, 2001, 5873, 1996, 2181, 1012, 2076, 2019, 4357, 2012, 2305, 2379, 1996, 5085, 1997, 102], [101, 2585, 24185, 22895, 10464, 21355, 1521, 1055, 10768, 2099, 15338, 14842, 2001, 16021, 13699, 25236, 2013, 2010, 2969, 1011, 23138, 14982, 2015, 1012, 9982, 2011, 2585, 24185, 22895, 10464, 21355, 1998, 3419, 6031, 1013, 14571, 1996, 3776, 1997, 2585, 24185, 22895, 10464, 21355, 1998, 1052, 1012, 1052, 1012, 102], [101, 1996, 26721, 16069, 2140, 7982, 2090, 1996, 2047, 2259, 2695, 1521, 1055, 2061, 13492, 2497, 6289, 7849, 2072, 1998, 2120, 3319, 1521, 1055, 2585, 2413, 4247, 2000, 4895, 10371, 1012, 2070, 2089, 4687, 2043, 2009, 2097, 2203, 1517, 2021, 2045, 2003, 2053, 2062, 2590, 7789, 5981, 1999, 2035, 102], [101, 3883, 12834, 2821, 2081, 1037, 4474, 3311, 2012, 1037, 1523, 2644, 4004, 5223, 1524, 6186, 1999, 6278, 1010, 3552, 1010, 2006, 5095, 1010, 2073, 2016, 2419, 1037, 16883, 4013, 27640, 1010, 1523, 1045, 2572, 7098, 2000, 2022, 4004, 1012, 1524, 1523, 6278, 1010, 1045, 2572, 2061, 3407, 1998, 102], [101, 2064, 2017, 6510, 1999, 1037, 2261, 14189, 2000, 2393, 4636, 2388, 3557, 1005, 15025, 8083, 1029, 2057, 1005, 2128, 1037, 14495, 1006, 2061, 2009, 1005, 1055, 4171, 1011, 2139, 8566, 6593, 7028, 1007, 1010, 1998, 8068, 2490, 3084, 2039, 2055, 2048, 1011, 12263, 1997, 2256, 5166, 1012, 2057, 102], [101, 6074, 1999, 8398, 1005, 1055, 6987, 2031, 2042, 2356, 2055, 26811, 2000, 2225, 5340, 3509, 1012, 2004, 2163, 4088, 2000, 8292, 28228, 12031, 2602, 3463, 2008, 7744, 1037, 3377, 2005, 2343, 1011, 11322, 3533, 7226, 2368, 1010, 2130, 2295, 6221, 8398, 8440, 1005, 1056, 15848, 2045, 2003, 2028, 102], [101, 11091, 7047, 10878, 2869, 1010, 1046, 1012, 1041, 1012, 1006, 12609, 1007, 1012, 1523, 2045, 1521, 1055, 2467, 1037, 15111, 2105, 2043, 2017, 2123, 1521, 1056, 2215, 2028, 1524, 1024, 2054, 7825, 2064, 6570, 1996, 2277, 2865, 3698, 1012, 1999, 2585, 6469, 7825, 1998, 4676, 1024, 8927, 2006, 102], [101, 3000, 1006, 9706, 1007, 1517, 3901, 1997, 3000, 1998, 2195, 2060, 4655, 1997, 2605, 2985, 2037, 2034, 5353, 2104, 1037, 3132, 3204, 10052, 5843, 7698, 1012, 2096, 1996, 2413, 2231, 7278, 1996, 3513, 2052, 2022, 2625, 9384, 2084, 1999, 1996, 2627, 1010, 1996, 5761, 2031, 2042, 6367, 2004, 102], [101, 1037, 19633, 5271, 7245, 1999, 1996, 29461, 3022, 9496, 2229, 3006, 2001, 22464, 2011, 1037, 5294, 16388, 2013, 2759, 14279, 1010, 21106, 2011, 6381, 3012, 5936, 2008, 2071, 1999, 29301, 2062, 3255, 1999, 2746, 2420, 1012, 1996, 16388, 3047, 2012, 1037, 2051, 2043, 13066, 2020, 2525, 5191, 2055, 102], [101, 2320, 1996, 2522, 17258, 1011, 2539, 4335, 3021, 4641, 2000, 1996, 4001, 1010, 1996, 1002, 2321, 6263, 11897, 2097, 2227, 2048, 2502, 5852, 1024, 2028, 24508, 1010, 1998, 2028, 2576, 1012, 2006, 1996, 24508, 2392, 1010, 8037, 2342, 2000, 8054, 1996, 4001, 3323, 12199, 1517, 2019, 1999, 1011, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 4098, 20997, 2052, 2031, 2357, 5989, 7483, 1012, 2044, 2197, 2733, 1521, 1055, 9252, 9424, 29216, 1010, 2043, 1996, 102], [101, 1996, 2190, 24095, 6209, 2283, 2003, 6230, 5095, 1998, 2057, 2215, 2017, 2000, 3693, 2149, 999, 2004, 12609, 3310, 2000, 1037, 2485, 1010, 3693, 16092, 6031, 1998, 9606, 5671, 1011, 6890, 1010, 2522, 1011, 6184, 1997, 1996, 2291, 4638, 16110, 2013, 1996, 3842, 1010, 2005, 1037, 2200, 2569, 102], [101, 17047, 4456, 20543, 2089, 12996, 3945, 2000, 5672, 2088, 1005, 1055, 2087, 2691, 17901, 6359, 1000, 2561, 3571, 1998, 5213, 1012, 1000, 2008, 1005, 1055, 2129, 4080, 14161, 14643, 1010, 1037, 17901, 7155, 2012, 1996, 2118, 1997, 10622, 1999, 13679, 9856, 1010, 5577, 1996, 4668, 1997, 6617, 2000, 102], [101, 4841, 2040, 3427, 4419, 2739, 1005, 4422, 11382, 13728, 13775, 2063, 4357, 2343, 6221, 8398, 2024, 23558, 2012, 1996, 1000, 4419, 1004, 2814, 1000, 2522, 1011, 3677, 1005, 1055, 1000, 7676, 2989, 1000, 1998, 1000, 4352, 1000, 1996, 2343, 2000, 11867, 7974, 1000, 4682, 2044, 4682, 2044, 4682, 102], [101, 1996, 16266, 3597, 4502, 2221, 2604, 1997, 22565, 2442, 2735, 2058, 17069, 1998, 21628, 9513, 6681, 2000, 4001, 3951, 4177, 2061, 2027, 2064, 6204, 2019, 15727, 1997, 1996, 12609, 2236, 2602, 1010, 1037, 3648, 5451, 1012, 16266, 3597, 4502, 2221, 6020, 2457, 3648, 10805, 2726, 2239, 5451, 2006, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 4108, 3537, 12411, 1012, 6798, 11582, 7432, 21052, 6367, 2110, 10643, 2006, 4465, 2851, 1010, 3038, 2009, 2001, 1037, 102], [101, 2292, 2256, 8845, 2393, 2017, 2191, 3168, 1997, 1996, 5005, 1024, 4942, 29234, 2000, 1996, 2388, 3557, 3679, 17178, 1998, 2131, 1037, 28667, 9331, 1997, 2739, 2008, 5609, 1012, 2043, 7713, 1011, 1999, 1011, 2173, 4449, 7260, 1996, 2406, 1999, 2233, 1010, 6683, 9581, 8038, 29045, 2050, 1521, 102], [101, 4108, 10643, 2031, 3107, 2664, 2178, 27284, 3021, 2008, 2052, 21040, 2689, 1996, 2110, 1005, 1055, 6830, 4277, 1010, 2023, 2051, 17903, 1037, 2048, 1011, 3931, 6378, 2055, 9962, 4402, 5097, 2046, 1037, 6109, 1011, 3931, 27284, 2207, 2019, 3178, 2077, 1037, 2837, 3116, 1012, 2004, 2009, 2979, 102], [101, 1037, 2662, 2450, 2001, 2022, 9250, 2011, 2019, 7904, 2012, 1037, 5806, 3806, 2276, 2023, 2733, 2005, 4092, 3009, 2000, 2178, 2450, 2040, 3047, 2000, 2022, 2551, 2045, 1012, 2334, 2739, 2276, 5925, 1021, 4311, 2008, 2624, 3533, 6319, 24665, 8586, 3148, 17866, 2253, 2046, 1037, 2334, 5806, 102], [101, 1996, 2739, 17848, 4803, 24304, 2015, 2862, 2097, 2079, 2062, 2084, 23216, 2115, 2568, 1012, 2122, 9631, 2089, 4119, 2115, 9029, 1010, 5041, 2368, 2115, 15251, 1010, 4654, 17847, 2115, 12731, 9488, 24279, 1010, 2030, 21255, 2115, 9647, 1012, 2122, 2808, 2089, 2025, 9352, 3711, 2006, 1996, 2880, 102], [101, 1996, 2034, 2792, 1997, 4363, 2039, 2007, 1996, 10556, 13639, 6182, 6962, 1010, 2029, 4836, 2006, 13323, 1012, 2403, 1010, 2289, 1010, 3216, 2074, 2104, 2570, 2781, 2302, 12698, 1012, 2009, 4269, 2007, 1996, 2155, 5378, 6805, 1011, 2946, 4955, 1997, 3209, 1006, 1047, 7317, 8913, 1024, 1523, 102], [101, 3462, 2150, 1037, 3595, 2653, 2005, 19050, 7179, 1010, 1998, 2009, 4247, 2000, 5050, 2304, 12969, 2646, 7931, 1012, 2004, 2019, 6388, 6254, 12199, 1010, 1045, 24964, 2863, 11528, 2063, 1996, 3853, 1997, 8264, 2005, 2304, 2308, 1012, 2045, 2024, 3441, 1996, 10748, 2031, 3950, 1010, 1998, 1045, 102], [101, 1043, 2135, 8458, 8820, 2618, 2003, 1037, 22575, 5041, 1011, 8674, 12810, 21752, 2008, 2003, 2011, 2085, 1996, 2087, 8077, 2109, 12810, 21752, 1999, 1996, 2088, 1998, 2038, 2042, 1996, 3120, 2005, 1037, 2145, 9685, 6704, 2055, 2049, 17631, 3896, 2006, 2529, 2740, 1998, 1996, 4044, 1012, 1996, 102], [101, 2531, 3263, 3998, 4278, 2454, 11703, 1012, 2410, 9388, 1012, 2538, 4008, 2620, 2454, 2561, 21656, 8564, 2062, 2084, 4008, 2620, 2454, 17404, 21656, 2031, 2042, 8564, 4969, 1010, 5020, 2000, 1019, 1012, 1022, 21656, 2005, 2296, 2531, 2111, 1012, 2045, 2003, 2525, 1037, 9762, 6578, 2090, 12436, 102], [101, 2005, 1037, 2146, 2051, 1010, 2057, 2024, 2409, 1010, 16498, 2038, 2042, 2437, 1996, 2088, 13726, 1012, 9217, 1996, 6355, 9755, 2530, 2163, 4703, 5333, 6917, 1010, 13350, 2031, 5224, 2008, 3521, 2090, 3130, 23689, 6590, 15061, 2594, 3741, 1517, 2087, 5546, 1999, 2885, 1517, 2038, 2042, 4719, 102], [101, 12620, 8648, 1024, 2007, 8740, 3070, 2624, 10514, 2226, 18712, 2072, 14620, 1010, 2510, 3138, 2058, 2231, 4372, 8017, 3351, 2023, 3746, 2000, 24679, 14408, 3258, 5164, 2121, 1013, 9617, 3527, 7630, 4034, 3081, 2131, 3723, 4871, 5164, 2121, 1013, 9617, 3527, 7630, 4034, 3081, 2131, 3723, 4871, 102], [101, 1000, 2057, 18067, 4375, 1996, 7226, 2368, 3447, 1996, 2087, 5851, 3675, 1999, 2381, 1012, 2035, 2027, 2018, 2000, 2079, 2001, 2562, 2023, 5744, 1011, 2770, 2291, 2006, 8285, 8197, 10994, 1012, 2612, 1010, 1999, 1996, 8487, 1997, 1037, 2074, 2261, 3134, 1010, 1996, 7226, 2368, 3447, 2038, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5b719-a174-44f8-94d5-d9a10117e226",
   "metadata": {},
   "source": [
    "### testing the tf_dataset_constructor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28d38f8a-d686-4258-88e6-8e41d1589caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset_constructor(tokens,\n",
    "                           y):\n",
    "    \"\"\"\n",
    "    Using the tokenized input from the text_tokenizer function, returns TensorFlow objects for use in the DistilBert model.\n",
    "    \"\"\"\n",
    "\n",
    "    tfdataset = tf.data.Dataset.from_tensor_slices((dict(tokens),y))\n",
    "        \n",
    "    return tfdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2000411f-db5c-4f09-a6cb-1d6e43249801",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdataset = tf_dataset_constructor(tokens, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "874c9928-60a2-4904-bc88-3f6d2b95ad70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(50,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(50,), dtype=tf.int32, name=None)}, TensorSpec(shape=(), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdb064-001d-4ee2-8e2f-49fe1362dc1b",
   "metadata": {},
   "source": [
    "### testing the train_test_split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e8750cd-7935-44e6-a080-23b2b32bdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X,\n",
    "                     tfdataset,\n",
    "                     test_split = TEST_SPLIT,\n",
    "                     batch_size = BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    This function splits the TensorFlow object created in the tf_dataset_constructor function into train and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_size = int(len(X) * (1-test_split))\n",
    "    \n",
    "    tfdataset = tfdataset.shuffle(len(X))\n",
    "    tfdataset_train = tfdataset.take(train_size)\n",
    "    tfdataset_test = tfdataset.skip(train_size)\n",
    "    \n",
    "    tfdataset_train = tfdataset_train.batch(batch_size)\n",
    "    tfdataset_test = tfdataset_test.batch(batch_size)\n",
    "\n",
    "    return tfdataset_train, tfdataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa87f412-4b97-4a4e-a8d2-0bc215cbb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdataset_train, tfdataset_test = train_test_split(x, tfdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6710c59-4314-4eea-9035-c2b96a87a401",
   "metadata": {},
   "source": [
    "### testing the ideology_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7cd3c0ed-e4cd-4a74-b391-9121d8628208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideology_model(tfdataset_train,\n",
    "                   model_name = \"distilbert-base-uncased\",\n",
    "                   learning_rate = LEARNING_RATE,\n",
    "                   batch_size = BATCH_SIZE,\n",
    "                   epochs = 2):\n",
    "    \"\"\"\n",
    "    Set up an run a DistilBert model on our TensorFlow training dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up model\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    # define loss function\n",
    "    loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # define optimizer to be used to minimise loss\n",
    "    optimizer = optimizers.Adam(learning_rate)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = loss,\n",
    "                  metrics = \"accuracy\")\n",
    "    \n",
    "    # fit model\n",
    "    model.fit(tfdataset_train, batch_size = batch_size, epochs = epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e259e4ef-d72e-44b5-b7c1-683d2862cefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "37/37 [==============================] - 27s 557ms/step - loss: 0.6563 - accuracy: 0.5811\n",
      "Epoch 2/2\n",
      "37/37 [==============================] - 21s 557ms/step - loss: 0.5245 - accuracy: 0.7297\n"
     ]
    }
   ],
   "source": [
    "model = ideology_model(tfdataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f7035-5e42-4366-adba-032dcbee218b",
   "metadata": {},
   "source": [
    "### testing the ideology_model_evaluator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7002256-21a8-443b-85c6-8c9b45a70154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideology_model_evaluator(model,\n",
    "                             tfdataset_test,\n",
    "                             batch_size = BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Evaluate our model on the TensorFlow test dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    benchmarks = model.evaluate(tfdataset_test, batch_size = batch_size, return_dict = True)\n",
    "    accuracy = benchmarks[\"accuracy\"]\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34e5a556-0520-480c-a686-7cb67c634f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 87ms/step - loss: 0.4382 - accuracy: 0.7895\n"
     ]
    }
   ],
   "source": [
    "accuracy = ideology_model_evaluator(model, tfdataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "919d99ca-feab-4f74-a40c-f871fb25e4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7894737124443054"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf4fd06-d12b-4953-b4e7-29e1416ba621",
   "metadata": {},
   "source": [
    "### testing the ideology_model_predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ffcd492-f602-41eb-aef9-c4293ce361fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideology_model_predictor(model,\n",
    "                             tokens):\n",
    "    \"\"\"\n",
    "    This function uses the model output from the ideology_model function to output the probabilities of each\n",
    "    individual article being left or right wing (0 = left wing, 1 = right wing). As the model spits out log odds\n",
    "    rather than probabilities, these also need to be converted in this function into probabilities\n",
    "    \"\"\"\n",
    "    # firstly create a TensorFlow version of our tokenized dataset without our y\n",
    "    tfdataset_no_y = tf.data.Dataset.from_tensor_slices(dict(tokens))\n",
    "\n",
    "    # use this to get out the logits for our model\n",
    "    pred_logits = model.predict(tfdataset_no_y)[0]\n",
    "\n",
    "    # convert these into probabilties\n",
    "    pred_probas = tf.nn.softmax(pred_logits).numpy()\n",
    "\n",
    "    return pred_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0ba3026-e93c-4f1b-950f-5d1059bb83c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 6s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_probas = ideology_model_predictor(model, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b17f5a75-e286-454a-86b6-79c4962f61d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5669385 , 0.43306145],\n",
       "       [0.73527414, 0.2647259 ],\n",
       "       [0.72163135, 0.27836865],\n",
       "       [0.76579237, 0.23420767],\n",
       "       [0.8356334 , 0.16436656],\n",
       "       [0.75917053, 0.24082953],\n",
       "       [0.5621511 , 0.4378489 ],\n",
       "       [0.705724  , 0.294276  ],\n",
       "       [0.852224  , 0.14777601],\n",
       "       [0.77797884, 0.22202122],\n",
       "       [0.71515155, 0.28484848],\n",
       "       [0.728418  , 0.271582  ],\n",
       "       [0.47692505, 0.5230749 ],\n",
       "       [0.6683714 , 0.33162865],\n",
       "       [0.6675117 , 0.3324883 ],\n",
       "       [0.5555026 , 0.4444974 ],\n",
       "       [0.5470466 , 0.4529534 ],\n",
       "       [0.6623875 , 0.3376125 ],\n",
       "       [0.74266773, 0.25733227],\n",
       "       [0.8023471 , 0.19765295],\n",
       "       [0.601964  , 0.39803594],\n",
       "       [0.6232342 , 0.37676582],\n",
       "       [0.73969495, 0.26030508],\n",
       "       [0.8576266 , 0.14237338],\n",
       "       [0.7687634 , 0.23123656],\n",
       "       [0.5749443 , 0.42505565],\n",
       "       [0.49305683, 0.5069431 ],\n",
       "       [0.7371209 , 0.26287907],\n",
       "       [0.5062145 , 0.49378544],\n",
       "       [0.81750435, 0.18249565],\n",
       "       [0.8327217 , 0.16727826],\n",
       "       [0.47228163, 0.5277183 ],\n",
       "       [0.7392036 , 0.26079646],\n",
       "       [0.65530354, 0.34469646],\n",
       "       [0.6797603 , 0.32023978],\n",
       "       [0.84248966, 0.15751034],\n",
       "       [0.49041867, 0.5095813 ],\n",
       "       [0.45219347, 0.5478065 ],\n",
       "       [0.8079432 , 0.19205672],\n",
       "       [0.7663072 , 0.23369281],\n",
       "       [0.4818311 , 0.5181689 ],\n",
       "       [0.7225881 , 0.27741188],\n",
       "       [0.6166007 , 0.38339934],\n",
       "       [0.7074008 , 0.2925991 ],\n",
       "       [0.8194765 , 0.18052348],\n",
       "       [0.83925694, 0.16074312],\n",
       "       [0.74061507, 0.25938493],\n",
       "       [0.7574157 , 0.24258426],\n",
       "       [0.57521445, 0.42478552],\n",
       "       [0.5527624 , 0.44723755],\n",
       "       [0.5427424 , 0.45725763],\n",
       "       [0.7122403 , 0.28775972],\n",
       "       [0.7389497 , 0.2610503 ],\n",
       "       [0.7143033 , 0.28569672],\n",
       "       [0.5051424 , 0.49485758],\n",
       "       [0.7274244 , 0.27257562],\n",
       "       [0.6991202 , 0.30087975],\n",
       "       [0.681957  , 0.31804296],\n",
       "       [0.573952  , 0.42604795],\n",
       "       [0.66975653, 0.3302435 ],\n",
       "       [0.818706  , 0.18129402],\n",
       "       [0.7223375 , 0.27766252],\n",
       "       [0.76208925, 0.2379108 ],\n",
       "       [0.6093607 , 0.3906393 ],\n",
       "       [0.7980984 , 0.20190157],\n",
       "       [0.6127339 , 0.38726607],\n",
       "       [0.5096479 , 0.49035206],\n",
       "       [0.7283618 , 0.27163824],\n",
       "       [0.5974104 , 0.4025897 ],\n",
       "       [0.4828514 , 0.5171486 ],\n",
       "       [0.80341494, 0.19658504],\n",
       "       [0.77376115, 0.22623886],\n",
       "       [0.77892023, 0.22107978],\n",
       "       [0.45155704, 0.54844296],\n",
       "       [0.55919194, 0.4408081 ],\n",
       "       [0.73298424, 0.26701578],\n",
       "       [0.845884  , 0.15411596],\n",
       "       [0.7508416 , 0.2491584 ],\n",
       "       [0.70088977, 0.2991103 ],\n",
       "       [0.7984956 , 0.20150438],\n",
       "       [0.7496494 , 0.25035068],\n",
       "       [0.8301417 , 0.16985822],\n",
       "       [0.8390522 , 0.16094783],\n",
       "       [0.72428006, 0.27571994],\n",
       "       [0.6907483 , 0.3092517 ],\n",
       "       [0.4818311 , 0.5181689 ],\n",
       "       [0.7111971 , 0.28880292],\n",
       "       [0.71784127, 0.2821588 ],\n",
       "       [0.66611445, 0.33388552],\n",
       "       [0.68199265, 0.31800738],\n",
       "       [0.6989305 , 0.3010695 ],\n",
       "       [0.49820265, 0.5017973 ],\n",
       "       [0.53414226, 0.4658577 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32091a1-3754-44d6-9232-26a7c87bace6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956337c-e87d-4fc7-8f36-fbe141e5a1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb901c69-12f2-44f4-aed3-c9f565c54c38",
   "metadata": {},
   "source": [
    "## investigating the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99105235-df27-46a0-98ee-dec2d781cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a5c21-b2a0-450d-9b7e-085f8a0d6d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13856a-59c5-4c6b-8ba2-df95d50be9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in the left df with the base URL, stripped of any \"www.\" prefix for clarity\n",
    "df_left[\"base_url\"] = df_left[\"link\"].apply(lambda x: urlsplit(x).netloc.removeprefix(\"www.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cb0dc-ccd6-450c-98ba-a8281ab145f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this column into a list\n",
    "left_urls = df_left[\"base_url\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614862ad-0b9f-490c-a481-5e2c96ec1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an alphabetically-sorted list of the unique base URLs by converting into a set\n",
    "unique_left_urls = sorted(list(set(left_urls)))\n",
    "len(unique_left_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ada9b-4fd7-47b1-8469-c695096646a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same below with the right df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe96675-dc07-46b3-9416-c3532e4e8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right[\"base_url\"] = df_right[\"link\"].apply(lambda x: urlsplit(x).netloc.removeprefix(\"www.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3c6a4-9151-4cb3-9d7e-a9c8f92f9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_urls = df_right[\"base_url\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf24b1-44fc-42c7-80b5-eb214063f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_right_urls = sorted(list(set(right_urls)))\n",
    "len(unique_right_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f6cd1-8dbc-4fe9-a961-2a39aca3622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if there is any overlap between the lists?\n",
    "overlap = set(unique_left_urls) & set(unique_right_urls)\n",
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ca5cc-148f-498c-9267-c30c3d1eb380",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b9e01-8cf9-4bdf-9374-8478df2a90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### can therefore see that there are 26 base URLs where some news articles are considered \"right\" and some are considered \"left\"\n",
    "# this seems a little odd - for example, the far-right publication Breitart is considered both \"left\" and \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612397d-1054-4c38-b895-a5e6480a966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_left_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4061b8-71e4-4c82-9f25-a3e3bcff8a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
